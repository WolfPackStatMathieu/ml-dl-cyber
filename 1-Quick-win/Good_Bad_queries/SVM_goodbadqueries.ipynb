{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n",
    "Les machines à vecteurs de support (SVM, pour Support Vector Machines) sont particulièrement efficaces pour les problèmes de classification à haute dimensionnalité, comme c'est souvent le cas avec les données textuelles transformées via TF-IDF. Les SVM sont bien connus pour leur capacité à créer une frontière de décision optimale (appelée hyperplan) qui maximise la marge entre les classes de données.\n",
    "\n",
    "Pour appliquer un SVM à vos données, vous pouvez utiliser la bibliothèque `scikit-learn`, qui offre une implémentation efficace à travers la classe `SVC` (C-Support Vector Classification). Voici comment vous pouvez mettre en œuvre et évaluer un SVM sur vos données:\n",
    "\n",
    "### 1. Importer les bibliothèques nécessaires\n",
    "Vous aurez besoin de `SVC` pour le modèle SVM et de quelques métriques pour évaluer les performances:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Création et entraînement du modèle SVM\n",
    "La création d'un modèle SVM avec des paramètres par défaut et son entraînement peut prendre du temps, surtout si la taille de l'ensemble des données est grande. Vous pouvez commencer avec les paramètres par défaut, puis ajuster selon les besoins:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prédiction et évaluation du modèle\n",
    "Après l'entraînement, utilisez le modèle pour faire des prédictions sur l'ensemble de test, puis évaluez les performances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédiction des étiquettes sur l'ensemble de test\n",
    "# y_pred_svm = svm_model.predict(X_test_tfidf)\n",
    "\n",
    "# # Calcul et affichage des métriques de performance\n",
    "# print(\"Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
    "# print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_svm))\n",
    "# print(\"Classification Report:\\n\", classification_report(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajustements possibles\n",
    "- **Kernel :** Le choix du noyau (`kernel`) est crucial. Les noyaux communs incluent `linear`, `poly`, `rbf`, `sigmoid`. Pour les données textuelles, `linear` est souvent un bon point de départ.\n",
    "- **C (paramètre de régularisation) :** Ajuster ce paramètre peut aider à contrôler le compromis entre l'atteinte d'une marge maximale et la minimisation de l'erreur de classification.\n",
    "- **gamma :** Ce paramètre définit l'influence d'un seul exemple d'entraînement, important surtout pour les noyaux non linéaires comme `rbf`.\n",
    "\n",
    "### Points à noter\n",
    "- **Scalabilité :** Les SVM peuvent ne pas être les plus rapides pour de très grands ensembles de données à cause de leur complexité de calcul. Dans ce cas, des stratégies comme réduire la taille de l'ensemble de données ou utiliser des versions plus simplifiées des SVM (comme LinearSVC) peuvent être envisagées.\n",
    "- **Interprétabilité :** Bien que très performants, les SVM ne fournissent pas une interprétabilité aussi directe que d'autres modèles (comme la régression logistique).\n",
    "\n",
    "N'hésitez pas à expérimenter avec ces paramètres pour optimiser les performances de votre modèle SVM sur votre problème spécifique. Si vous avez besoin d'aide pour ajuster ces paramètres ou pour comprendre leurs effets, je suis là pour aider !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Mise en place de K-nearest neighbors\n",
    "KNN est un algorithme d'apprentissage supervisé simple et souvent efficace, qui prédit l'étiquette d'une donnée en se basant sur les étiquettes des \"K\" échantillons les plus proches dans l'espace des caractéristiques.\n",
    "\n",
    "### Considérations pour l'utilisation de KNN avec des données textuelles\n",
    "\n",
    "- **Dimensionnalité et performance** : KNN peut être moins efficace en termes de temps de calcul et de mémoire sur des ensembles de données de haute dimension, comme c'est souvent le cas avec les données textuelles transformées par TF-IDF. La réduction de dimension avec des techniques comme PCA (Principal Component Analysis) pourrait aider à améliorer les performances.\n",
    "- **Choix de K** : Le nombre de voisins, K, est un paramètre crucial. Un K trop petit peut rendre le modèle sensible au bruit, tandis qu'un K trop grand peut le rendre insensible aux structures réellement présentes dans les données. Il peut être utile de tester plusieurs valeurs de K pour trouver un bon équilibre.\n",
    "- **Métrique de distance** : La métrique utilisée pour calculer la distance entre les points est également importante. La distance euclidienne est standard, mais pour les données textuelles, d'autres métriques comme la distance de cosine pourraient parfois être plus appropriées.\n",
    "\n",
    "Expérimentez avec ces paramètres pour voir comment ils influencent les performances du modèle KNN sur vos données. Si vous avez besoin d'aide avec ces ajustements ou d'autres questions sur KNN, n'hésitez pas à demander !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oui, effectuer une réduction de dimension avant d'utiliser l'algorithme K-nearest neighbors (KNN) est une très bonne idée, surtout pour des données de haute dimension comme celles transformées par TF-IDF. La réduction de dimension peut améliorer significativement la performance de calcul de KNN et potentiellement aussi la qualité de la classification, en réduisant le bruit et en mettant en évidence les structures sous-jacentes des données.\n",
    "\n",
    "### Utilisation de PCA (Principal Component Analysis)\n",
    "\n",
    "Une méthode populaire de réduction de dimension est l'analyse en composantes principales (PCA), qui est souvent utilisée pour diminuer le nombre de dimensions tout en conservant le plus d'information possible. Voici comment vous pouvez appliquer PCA à vos données textuelles vectorisées avant de les utiliser pour KNN :\n",
    "\n",
    "1. **Importation des bibliothèques nécessaires** :\n",
    "   Vous aurez besoin de `PCA` de la bibliothèque `scikit-learn` ainsi que des autres composants utilisés précédemment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Application de PCA** :\n",
    "   Vous devrez choisir le nombre de composantes principales à conserver. Ce choix peut dépendre de la variabilité des données que vous souhaitez conserver.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "\u001b[1;32m/home/onyxia/work/ml-dl-cyber/1-Quick-win/Good_Bad_queries/exploration_good_bad_queries.ipynb Cell 43\u001b[0m line \u001b[0;36m7\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell://user-mthomassin-246603-0.user.lab.sspcloud.fr/home/onyxia/work/ml-dl-cyber/1-Quick-win/Good_Bad_queries/exploration_good_bad_queries.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m svd \u001b[39m=\u001b[39m TruncatedSVD(n_components\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell://user-mthomassin-246603-0.user.lab.sspcloud.fr/home/onyxia/work/ml-dl-cyber/1-Quick-win/Good_Bad_queries/exploration_good_bad_queries.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Adapter TruncatedSVD aux données d'entraînement et les transformer\u001b[39;00m\n",
      "\u001b[0;32m----> <a href='vscode-notebook-cell://user-mthomassin-246603-0.user.lab.sspcloud.fr/home/onyxia/work/ml-dl-cyber/1-Quick-win/Good_Bad_queries/exploration_good_bad_queries.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m X_train_svd \u001b[39m=\u001b[39m svd\u001b[39m.\u001b[39;49mfit_transform(X_train_tfidf)\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell://user-mthomassin-246603-0.user.lab.sspcloud.fr/home/onyxia/work/ml-dl-cyber/1-Quick-win/Good_Bad_queries/exploration_good_bad_queries.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Transformer également les données de test en utilisant le même transformateur\u001b[39;00m\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://user-mthomassin-246603-0.user.lab.sspcloud.fr/home/onyxia/work/ml-dl-cyber/1-Quick-win/Good_Bad_queries/exploration_good_bad_queries.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m X_test_svd \u001b[39m=\u001b[39m svd\u001b[39m.\u001b[39mtransform(X_test_tfidf)\n",
      "\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.12/site-packages/sklearn/utils/_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    293\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n",
      "\u001b[1;32m    294\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n",
      "\u001b[0;32m--> 295\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m    296\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n",
      "\u001b[1;32m    297\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n",
      "\u001b[1;32m    298\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n",
      "\u001b[1;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n",
      "\u001b[1;32m    300\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n",
      "\u001b[1;32m    301\u001b[0m         )\n",
      "\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.12/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1467\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n",
      "\u001b[1;32m   1469\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n",
      "\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n",
      "\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n",
      "\u001b[1;32m   1472\u001b[0m     )\n",
      "\u001b[1;32m   1473\u001b[0m ):\n",
      "\u001b[0;32m-> 1474\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.12/site-packages/sklearn/decomposition/_truncated_svd.py:246\u001b[0m, in \u001b[0;36mTruncatedSVD.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n",
      "\u001b[1;32m    241\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_components \u001b[39m>\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]:\n",
      "\u001b[1;32m    242\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n",
      "\u001b[1;32m    243\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mn_components(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_components\u001b[39m}\u001b[39;00m\u001b[39m) must be <=\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;32m    244\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m n_features(\u001b[39m\u001b[39m{\u001b[39;00mX\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m).\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;32m    245\u001b[0m         )\n",
      "\u001b[0;32m--> 246\u001b[0m     U, Sigma, VT \u001b[39m=\u001b[39m randomized_svd(\n",
      "\u001b[1;32m    247\u001b[0m         X,\n",
      "\u001b[1;32m    248\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_components,\n",
      "\u001b[1;32m    249\u001b[0m         n_iter\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_iter,\n",
      "\u001b[1;32m    250\u001b[0m         n_oversamples\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_oversamples,\n",
      "\u001b[1;32m    251\u001b[0m         power_iteration_normalizer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpower_iteration_normalizer,\n",
      "\u001b[1;32m    252\u001b[0m         random_state\u001b[39m=\u001b[39;49mrandom_state,\n",
      "\u001b[1;32m    253\u001b[0m     )\n",
      "\u001b[1;32m    255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcomponents_ \u001b[39m=\u001b[39m VT\n",
      "\u001b[1;32m    257\u001b[0m \u001b[39m# As a result of the SVD approximation error on X ~ U @ Sigma @ V.T,\u001b[39;00m\n",
      "\u001b[1;32m    258\u001b[0m \u001b[39m# X @ V is not the same as U @ Sigma\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    184\u001b[0m global_skip_validation \u001b[39m=\u001b[39m get_config()[\u001b[39m\"\u001b[39m\u001b[39mskip_parameter_validation\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[1;32m    185\u001b[0m \u001b[39mif\u001b[39;00m global_skip_validation:\n",
      "\u001b[0;32m--> 186\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m    188\u001b[0m func_sig \u001b[39m=\u001b[39m signature(func)\n",
      "\u001b[1;32m    190\u001b[0m \u001b[39m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.12/site-packages/sklearn/utils/extmath.py:523\u001b[0m, in \u001b[0;36mrandomized_svd\u001b[0;34m(M, n_components, n_oversamples, n_iter, power_iteration_normalizer, transpose, flip_sign, random_state, svd_lapack_driver)\u001b[0m\n",
      "\u001b[1;32m    519\u001b[0m \u001b[39mif\u001b[39;00m transpose:\n",
      "\u001b[1;32m    520\u001b[0m     \u001b[39m# this implementation is a bit faster with smaller shape[1]\u001b[39;00m\n",
      "\u001b[1;32m    521\u001b[0m     M \u001b[39m=\u001b[39m M\u001b[39m.\u001b[39mT\n",
      "\u001b[0;32m--> 523\u001b[0m Q \u001b[39m=\u001b[39m randomized_range_finder(\n",
      "\u001b[1;32m    524\u001b[0m     M,\n",
      "\u001b[1;32m    525\u001b[0m     size\u001b[39m=\u001b[39;49mn_random,\n",
      "\u001b[1;32m    526\u001b[0m     n_iter\u001b[39m=\u001b[39;49mn_iter,\n",
      "\u001b[1;32m    527\u001b[0m     power_iteration_normalizer\u001b[39m=\u001b[39;49mpower_iteration_normalizer,\n",
      "\u001b[1;32m    528\u001b[0m     random_state\u001b[39m=\u001b[39;49mrandom_state,\n",
      "\u001b[1;32m    529\u001b[0m )\n",
      "\u001b[1;32m    531\u001b[0m \u001b[39m# project M to the (k + p) dimensional space using the basis vectors\u001b[39;00m\n",
      "\u001b[1;32m    532\u001b[0m B \u001b[39m=\u001b[39m Q\u001b[39m.\u001b[39mT \u001b[39m@\u001b[39m M\n",
      "\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.12/site-packages/sklearn/utils/extmath.py:345\u001b[0m, in \u001b[0;36mrandomized_range_finder\u001b[0;34m(A, size, n_iter, power_iteration_normalizer, random_state)\u001b[0m\n",
      "\u001b[1;32m    341\u001b[0m     Q, _ \u001b[39m=\u001b[39m normalizer(A\u001b[39m.\u001b[39mT \u001b[39m@\u001b[39m Q)\n",
      "\u001b[1;32m    343\u001b[0m \u001b[39m# Sample the range of A using by linear projection of Q\u001b[39;00m\n",
      "\u001b[1;32m    344\u001b[0m \u001b[39m# Extract an orthonormal basis\u001b[39;00m\n",
      "\u001b[0;32m--> 345\u001b[0m Q, _ \u001b[39m=\u001b[39m qr_normalizer(A \u001b[39m@\u001b[39;49m Q)\n",
      "\u001b[1;32m    347\u001b[0m \u001b[39mreturn\u001b[39;00m Q\n",
      "\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.12/site-packages/scipy/sparse/_base.py:695\u001b[0m, in \u001b[0;36m_spbase.__matmul__\u001b[0;34m(self, other)\u001b[0m\n",
      "\u001b[1;32m    692\u001b[0m \u001b[39mif\u001b[39;00m isscalarlike(other):\n",
      "\u001b[1;32m    693\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mScalar operands are not allowed, \u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;32m    694\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39muse \u001b[39m\u001b[39m'\u001b[39m\u001b[39m*\u001b[39m\u001b[39m'\u001b[39m\u001b[39m instead\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;32m--> 695\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_matmul_dispatch(other)\n",
      "\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.12/site-packages/scipy/sparse/_base.py:595\u001b[0m, in \u001b[0;36m_spbase._matmul_dispatch\u001b[0;34m(self, other)\u001b[0m\n",
      "\u001b[1;32m    593\u001b[0m         \u001b[39mreturn\u001b[39;00m result\u001b[39m.\u001b[39mreshape(M, \u001b[39m1\u001b[39m)\n",
      "\u001b[1;32m    594\u001b[0m     \u001b[39melif\u001b[39;00m other\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m other\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m N:\n",
      "\u001b[0;32m--> 595\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_matmul_multivector(other)\n",
      "\u001b[1;32m    597\u001b[0m \u001b[39mif\u001b[39;00m isscalarlike(other):\n",
      "\u001b[1;32m    598\u001b[0m     \u001b[39m# scalar value\u001b[39;00m\n",
      "\u001b[1;32m    599\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mul_scalar(other)\n",
      "\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.12/site-packages/scipy/sparse/_compressed.py:499\u001b[0m, in \u001b[0;36m_cs_matrix._matmul_multivector\u001b[0;34m(self, other)\u001b[0m\n",
      "\u001b[1;32m    497\u001b[0m \u001b[39m# csr_matvecs or csc_matvecs\u001b[39;00m\n",
      "\u001b[1;32m    498\u001b[0m fn \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(_sparsetools, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_matvecs\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;32m--> 499\u001b[0m fn(M, N, n_vecs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindptr, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata,\n",
      "\u001b[1;32m    500\u001b[0m    other\u001b[39m.\u001b[39;49mravel(), result\u001b[39m.\u001b[39;49mravel())\n",
      "\u001b[1;32m    502\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Création d'une instance de TruncatedSVD pour réduire à 100 dimensions\n",
    "svd = TruncatedSVD(n_components=100)\n",
    "\n",
    "# Adapter TruncatedSVD aux données d'entraînement et les transformer\n",
    "X_train_svd = svd.fit_transform(X_train_tfidf)\n",
    "\n",
    "# Transformer également les données de test en utilisant le même transformateur\n",
    "X_test_svd = svd.transform(X_test_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Entraînement et évaluation du modèle KNN** :\n",
    "   Entraînez ensuite votre modèle KNN sur les données réduites.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Exemple de préparation des données\n",
    "X_train_tfidf_dense = np.array(X_train_tfidf.todense())  # Conversion de sparse à dense\n",
    "\n",
    "# Supposons que vous avez déjà divisé vos données:\n",
    "# X_train, X_test, y_train, y_test\n",
    "\n",
    "# Puis, par exemple pour un modèle RNN :\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "model_rnn = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_tfidf_dense.shape[1],)),\n",
    "    LSTM(32),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_rnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_rnn.fit(X_train_tfidf_dense, y_train, epochs=10, batch_size=32)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
