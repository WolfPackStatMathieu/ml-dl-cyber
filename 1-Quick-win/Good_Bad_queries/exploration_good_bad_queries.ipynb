{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour charger et préparer vos fichiers `badqueries.txt` et `goodqueries.txt` pour l'entraînement d'un modèle de machine learning avec Python et scikit-learn, vous pouvez suivre les étapes ci-dessous. Nous allons lire les fichiers, les étiqueter, les combiner en un seul dataframe, puis les préparer pour le processus d'apprentissage.\n",
    "\n",
    "### Étape 1 : Charger les données\n",
    "\n",
    "Assurez-vous que Python peut accéder aux fichiers où ils sont stockés sur votre système. Si nécessaire, ajustez le chemin d'accès aux fichiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Charger les données\n",
    "def load_data(filepath, label):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        data = [(line.strip(), label) for line in lines if line.strip()]\n",
    "    return data\n",
    "\n",
    "# Emplacements des fichiers\n",
    "bad_queries_path = 'badqueries.txt'\n",
    "good_queries_path = 'goodqueries.txt'\n",
    "\n",
    "# Charger et étiqueter les données\n",
    "bad_data = load_data(bad_queries_path, 1)  # Étiquette 1 pour les mauvaises requêtes\n",
    "good_data = load_data(good_queries_path, 0)  # Étiquette 0 pour les bonnes requêtes\n",
    "\n",
    "# Combinez les données dans un DataFrame\n",
    "all_data = pd.DataFrame(bad_data + good_data, columns=['query', 'label'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Petite exploration des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['query', 'label'], dtype='object')\n",
      "(1342596, 2)\n",
      "                                               query  label\n",
      "0                 /top.php?stuff='uname >q36497765 #      1\n",
      "1  /h21y8w52.nsf?<script>cross_site_scripting.nas...      1\n",
      "2  /ca000001.pl?action=showcart&hop=\\\"><script>al...      1\n",
      "3  /scripts/edit_image.php?dn=1&userfile=/etc/pas...      1\n",
      "4                                /javascript/mta.exe      1\n"
     ]
    }
   ],
   "source": [
    "print(all_data.columns)\n",
    "print(all_data.shape)\n",
    "print(all_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Étape 2 : Préparation des données\n",
    "\n",
    "Nous diviserons les données en ensembles d'entraînement et de test, puis appliquerons la vectorisation TF-IDF pour convertir les requêtes textuelles en vecteurs numériques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Séparer les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_data['query'], all_data['label'], test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/' '00' '02' '04' '0f' '40' '4d' '50' '5a' '<' '=' '>' 'allwords' 'b8'\n",
      " 'br' 'cal' 'cid' 'db' 'desc' 'echo' 'esbq' 'examples' 'ff' 'filter' 'foo'\n",
      " 'formdata' 'jsp' 'moodle' 'pathname' 'php' 'script' 'search' 'tex'\n",
      " 'texed' 'title']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    # Utilisez une expression régulière pour considérer les mots et certains symboles spéciaux\n",
    "    token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b|\\/|<|>|script|alert|http|\\.exe|\\.jpg|=\")\n",
    "    return token_pattern.findall(text)\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer, lowercase=False, strip_accents=None)\n",
    "\n",
    "# Exemple d'utilisation\n",
    "example_data = [\n",
    "    \"/examples/jsp/cal/search.php?allwords=<br><script>foo</script>&cid=0&title=1&desc=1\",\n",
    "    \"/moodle/filter/tex/texed.php?formdata=foo&pathname=foo\\\"+||+echo+db+4d+5a+50+00+02+00+00+00+04+00+0f+00+ff+ff+00+00+b8+00+00+00+00+00+00+00+40++>>esbq\"\n",
    "]\n",
    "\n",
    "X = vectorizer.fit_transform(example_data)\n",
    "print(vectorizer.get_feature_names_out())  # Affiche les termes du vocabulaire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorisation TF-IDF\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour comprendre à quoi ressemble `X_train_tfidf` après vectorisation avec TF-IDF, vous pouvez visualiser quelques aspects de cette matrice. `X_train_tfidf` est une matrice sparse (creuse) de `scikit-learn`, ce qui signifie qu'elle stocke uniquement les emplacements et les valeurs des éléments non nuls pour économiser de l'espace mémoire.\n",
    "\n",
    "### 1. Propriétés de base de la matrice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_tfidf: (1074076, 755132)\n",
      "Type of X_train_tfidf: <class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train_tfidf:\", X_train_tfidf.shape)  # dimensions de la matrice\n",
    "print(\"Type of X_train_tfidf:\", type(X_train_tfidf))   # type de l'objet (sparse matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Vérifier le nombre d'éléments non nuls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-zero elements: 3318992\n",
      "Density of the matrix: 4.0921196018022575e-06\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of non-zero elements:\", X_train_tfidf.nnz)\n",
    "print(\"Density of the matrix:\", X_train_tfidf.nnz / float(X_train_tfidf.shape[0] * X_train_tfidf.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Affichage des termes du vocabulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some feature names: ['.exe' '.jpg' '/' '00' '000' '0000' '000000' '0000000' '00000000'\n",
      " '000000000' '00000000000000000020195001' '00000000000000097vascript'\n",
      " '000000000001' '0000000001' '0000000001_000000000000000051240'\n",
      " '0000000001_000000000000000122701' '000000000663' '000000000667'\n",
      " '000000000699' '000000000788']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"Some feature names:\", feature_names[:20])  # affiche les 20 premiers termes du vocabulaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Votre mise en œuvre du `TfidfVectorizer` avec un tokenizer personnalisé semble bien adaptée pour gérer les requêtes potentiellement malveillantes que vous avez mentionnées. Vous avez bien configuré les étapes pour diviser vos données et appliquer la vectorisation TF-IDF, ce qui est crucial pour la suite de l'analyse avec un modèle de machine learning.\n",
    "\n",
    "### Points à considérer :\n",
    "1. **Validation du Tokenizer :** Assurez-vous que le tokenizer personnalisé extrait bien les termes pertinents de vos données d'entraînement et de test. Les termes imprimés depuis `example_data` donnent une bonne idée des tokens que votre vectorisateur considère.\n",
    "\n",
    "2. **Entraînement et transformation :** Vous avez utilisé `fit_transform` sur `X_train` et `transform` sur `X_test`, ce qui est la méthode correcte pour éviter la fuite de données de l'ensemble de test dans le modèle de vectorisation.\n",
    "\n",
    "3. **Gestion des caractères et motifs spéciaux :** Votre expression régulière semble bien conçue pour capturer à la fois les mots standards et les motifs spécifiques aux scripts malveillants, ce qui est essentiel pour ce type d'analyse.\n",
    "\n",
    "### Étapes suivantes :\n",
    "\n",
    "Maintenant que vos données sont prêtes, l'étape suivante consiste à choisir et entraîner un modèle de machine learning pour classifier les requêtes comme bonnes ou mauvaises. Voici quelques options de modèles que vous pourriez envisager :\n",
    "\n",
    "- **Régression logistique :** Un modèle simple mais efficace pour les tâches de classification binaire.\n",
    "- **Machines à vecteurs de support (SVM) :** Très efficace pour les espaces de grande dimension comme ceux créés par TF-IDF.\n",
    "- **Forêts aléatoires :** Bon pour gérer des données avec beaucoup de variance et pour capturer des non-linéarités sans trop de tuning nécessaire.\n",
    "- **Réseaux de neurones :** Si vous avez suffisamment de données et de ressources de calcul, un réseau de neurones pourrait capter des interactions complexes entre les mots.\n",
    "\n",
    "Chaque modèle a ses avantages et ses inconvénients, et le choix peut dépendre de la taille de vos données, de la précision requise, du temps de calcul disponible, et de votre familiarité avec les modèles.\n",
    "\n",
    "Si vous souhaitez des conseils sur l'entraînement d'un modèle spécifique ou sur l'évaluation de la performance du modèle, n'hésitez pas à demander !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Régression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9944212721584984\n",
      "Confusion Matrix:\n",
      " [[258575    446]\n",
      " [  1052   8447]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    259021\n",
      "           1       0.95      0.89      0.92      9499\n",
      "\n",
      "    accuracy                           0.99    268520\n",
      "   macro avg       0.97      0.94      0.96    268520\n",
      "weighted avg       0.99      0.99      0.99    268520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Création du modèle de régression logistique\n",
    "logreg = LogisticRegression(random_state=42)\n",
    "\n",
    "# Entraînement du modèle avec les données d'entraînement\n",
    "logreg.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Prédiction des étiquettes sur l'ensemble de test\n",
    "y_pred = logreg.predict(X_test_tfidf)\n",
    "\n",
    "# Calcul et affichage des métriques de performance\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Votre modèle de régression logistique semble obtenir d'excellents résultats globalement, comme le montre la très haute précision globale (accuracy) de 99.44%. Cependant, il est toujours utile de regarder un peu plus en détail chaque métrique pour bien comprendre les performances du modèle, surtout dans le contexte de la classification de requêtes potentiellement malveillantes.\n",
    "\n",
    "### Analyse des Résultats\n",
    "\n",
    "1. **Précision globale (Accuracy) :** À 99.44%, la plupart des prédictions sont correctes. C'est une bonne indication que le modèle fonctionne bien en général.\n",
    "\n",
    "2. **Matrice de confusion :** \n",
    "   - **Vrais positifs pour la classe 0 (bonnes requêtes)** : 258575\n",
    "   - **Faux positifs pour la classe 0** : 446\n",
    "   - **Faux négatifs pour la classe 0** : 1052\n",
    "   - **Vrais positifs pour la classe 1 (mauvaises requêtes)** : 8447\n",
    "\n",
    "   Les faibles nombres de faux positifs et de faux négatifs suggèrent que le modèle est à la fois précis et sensible. Cependant, le nombre de faux négatifs est supérieur aux faux positifs, ce qui indique une légère faiblesse dans la capture de toutes les mauvaises requêtes (moins de sensibilité).\n",
    "\n",
    "3. **Rapport de classification :**\n",
    "   - **Précision pour la classe 0** : Pratiquement parfaite à 1.00.\n",
    "   - **Précision pour la classe 1** : À 0.95, c'est très bon, mais cela indique aussi que certains faux positifs sont présents.\n",
    "   - **Rappel pour la classe 0** : Aussi pratiquement parfait à 1.00.\n",
    "   - **Rappel pour la classe 1** : À 0.89, ce qui indique que le modèle manque environ 11% des mauvaises requêtes réelles.\n",
    "   - **F1-score pour la classe 1** : À 0.92, ce score équilibré entre précision et rappel est assez bon, mais peut être amélioré, surtout si l'identification précise des mauvaises requêtes est critique.\n",
    "\n",
    "### Suggestions pour l'amélioration\n",
    "\n",
    "- **Ajustement des hyperparamètres :** Vous pouvez tenter d'ajuster le paramètre de régularisation `C` de la régression logistique pour voir si une valeur plus faible (plus de régularisation) ou plus élevée (moins de régularisation) améliore les performances.\n",
    "\n",
    "- **Équilibrage des classes :** Si votre jeu de données est très déséquilibré, envisagez des techniques comme le rééchantillonnage (oversampling de la classe minoritaire ou undersampling de la classe majoritaire) ou l'utilisation de poids de classe dans le modèle de régression logistique pour donner plus de poids à la classe minoritaire pendant l'entraînement.\n",
    "\n",
    "- **Validation croisée :** Utiliser une validation croisée pour évaluer la robustesse du modèle sur différents sous-ensembles de vos données peut aider à garantir que les performances ne sont pas spécifiques à une division particulière des données.\n",
    "\n",
    "- **Essayer d'autres modèles :** Parfois, un modèle différent peut capturer les dynamiques des données plus efficacement. Les machines à vecteurs de support (SVM), les forêts aléatoires ou même un modèle de réseaux de neurones pourraient être testés pour comparer les performances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prédiction avec des données issues de Splunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: web4gfo.preprod.insee.fr:443/fr/graphique/affaires\n",
      "Prediction: Good Query\n",
      "Prediction Probability: [0.99601602 0.00398398]\n"
     ]
    }
   ],
   "source": [
    "# Prédire avec une nouvelle query\n",
    "new_query = [\"web4gfo.preprod.insee.fr:443/fr/graphique/affaires\"]\n",
    "new_query_tfidf = vectorizer.transform(new_query)\n",
    "prediction = logreg.predict(new_query_tfidf)\n",
    "prediction_proba = logreg.predict_proba(new_query_tfidf)\n",
    "\n",
    "print(f\"Query: {new_query[0]}\")\n",
    "print(f\"Prediction: {'Bad Query' if prediction[0] == 1 else 'Good Query'}\")\n",
    "print(f\"Prediction Probability: {prediction_proba[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le résultat de la prédiction peut être interprété comme suit :\n",
    "\n",
    "### Query:\n",
    "`web4gfo.preprod.insee.fr:443/fr/graphique/affaires`\n",
    "\n",
    "Cette est la requête que nous avons soumise pour la prédiction.\n",
    "\n",
    "### Prediction:\n",
    "`Good Query`\n",
    "\n",
    "Le modèle a prédit que cette requête est une \"Good Query\" (bonne requête). Cela signifie que, selon le modèle, cette requête n'est pas malveillante ou problématique.\n",
    "\n",
    "### Prediction Probability:\n",
    "`[0.99601602 0.00398398]`\n",
    "\n",
    "Ce tableau de probabilités fournit des informations supplémentaires sur la confiance du modèle dans sa prédiction :\n",
    "- **0.99601602** (99.60%) : C'est la probabilité que la requête soit une bonne requête.\n",
    "- **0.00398398** (0.40%) : C'est la probabilité que la requête soit une mauvaise requête.\n",
    "\n",
    "### Interprétation:\n",
    "\n",
    "Le modèle est très confiant (99.60%) que la requête `web4gfo.preprod.insee.fr:443/fr/graphique/affaires` est une bonne requête. La très faible probabilité de 0.40% que cette requête soit une mauvaise requête indique que le modèle a peu de doutes quant à sa classification.\n",
    "\n",
    "En résumé, selon le modèle de régression logistique, cette requête est presque certainement bénigne et non malveillante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Chargement et préparation des données**:\n",
    "   - Les fichiers `badqueries.txt` et `goodqueries.txt` sont chargés et étiquetés.\n",
    "   - Les données sont combinées dans un DataFrame et divisées en ensembles d'entraînement et de test.\n",
    "\n",
    "2. **Tokenisation personnalisée**:\n",
    "   - Une fonction de tokenisation personnalisée est définie pour extraire des tokens spécifiques des requêtes.\n",
    "\n",
    "3. **Vectorisation TF-IDF**:\n",
    "   - Les données sont transformées en vecteurs TF-IDF en utilisant la fonction de tokenisation personnalisée.\n",
    "\n",
    "4. **Modèle de régression logistique**:\n",
    "   - Un modèle de régression logistique est formé sur les données vectorisées.\n",
    "\n",
    "5. **Prédiction avec une nouvelle query**:\n",
    "   - La nouvelle query est transformée en vecteur TF-IDF et utilisée pour prédire si elle est une \"bonne\" ou \"mauvaise\" requête.\n",
    "\n",
    "6. **Affichage des résultats**:\n",
    "   - La prédiction et les probabilités associées sont affichées.\n",
    "   - Les métriques de performance sur les données de test sont également affichées pour évaluer le modèle.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour améliorer la précision d'un modèle de régression logistique ou tout autre modèle de classification, plusieurs stratégies peuvent être mises en œuvre. Voici quelques-unes des méthodes courantes :\n",
    "\n",
    "### 1. **Collecte et Préparation des Données**\n",
    "- **Augmenter la Taille du Jeu de Données** : Plus de données peuvent aider le modèle à mieux généraliser.\n",
    "- **Équilibrer le Jeu de Données** : Assurez-vous que les classes sont équilibrées pour éviter le biais vers la classe majoritaire.\n",
    "- **Nettoyage des Données** : Supprimez les valeurs aberrantes et corrigez les erreurs dans les données.\n",
    "\n",
    "### 2. **Feature Engineering**\n",
    "- **Créer de Nouvelles Caractéristiques** : Identifier et ajouter de nouvelles caractéristiques pertinentes à partir des données existantes.\n",
    "- **Sélection de Caractéristiques** : Utilisez des techniques comme la sélection de caractéristiques (Feature Selection) pour garder seulement les plus pertinentes.\n",
    "- **Transformation des Caractéristiques** : Appliquer des transformations logarithmiques ou de normalisation aux caractéristiques numériques.\n",
    "\n",
    "### 3. **Optimisation du Modèle**\n",
    "- **Ajustement des Hyperparamètres** : Utilisez des techniques comme la recherche en grille (Grid Search) ou la recherche aléatoire (Random Search) pour trouver les meilleurs hyperparamètres.\n",
    "- **Validation Croisée** : Utilisez la validation croisée pour évaluer la performance du modèle de manière plus robuste.\n",
    "\n",
    "### 4. **Enrichissement des Caractéristiques Textuelles**\n",
    "- **Utiliser des Modèles de Langage Avancés** : Envisagez d'utiliser des modèles comme BERT ou TF-IDF pondéré.\n",
    "- **Améliorer la Tokenisation** : Utilisez des méthodes de tokenisation plus sophistiquées qui prennent en compte le contexte des mots.\n",
    "\n",
    "### 5. **Ensembles de Modèles**\n",
    "- **Ensemble Learning** : Combinez les prédictions de plusieurs modèles pour améliorer la robustesse et la précision (par exemple, en utilisant Bagging, Boosting ou Stacking).\n",
    "\n",
    "### 6. **Algorithmes Alternatifs**\n",
    "- **Essayer d'Autres Algorithmes** : Envisagez d'utiliser d'autres modèles comme les forêts aléatoires, les SVMs, les réseaux neuronaux, etc.\n",
    "\n",
    "### Exemple d'Amélioration\n",
    "\n",
    "Voici un exemple de code illustrant certaines de ces améliorations, en particulier l'optimisation des hyperparamètres et l'utilisation de validation croisée :\n",
    "\n",
    "\n",
    "### Explications du Code :\n",
    "\n",
    "1. **Pipeline** : Utilisation de `Pipeline` pour combiner le vecteur TF-IDF et le modèle de régression logistique.\n",
    "2. **Recherche en Grille** : Définition d'une grille de paramètres à tester pour trouver les meilleurs hyperparamètres.\n",
    "3. **Validation Croisée** : Utilisation de la validation croisée pour évaluer la performance du modèle avec différentes configurations.\n",
    "4. **Prédiction** : Utilisation du meilleur modèle trouvé pour prédire la classe et les probabilités pour une nouvelle requête.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/mamba/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                        TfidfVectorizer(lowercase=False,\n",
       "                                                        tokenizer=&lt;function custom_tokenizer at 0x7f366dd240e0&gt;)),\n",
       "                                       (&#x27;classifier&#x27;, LogisticRegression())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;classifier__C&#x27;: [0.1, 1, 10],\n",
       "                         &#x27;classifier__solver&#x27;: [&#x27;liblinear&#x27;, &#x27;saga&#x27;],\n",
       "                         &#x27;vectorizer__max_df&#x27;: [0.75, 0.85, 1.0],\n",
       "                         &#x27;vectorizer__min_df&#x27;: [1, 2, 3]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;GridSearchCV<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                        TfidfVectorizer(lowercase=False,\n",
       "                                                        tokenizer=&lt;function custom_tokenizer at 0x7f366dd240e0&gt;)),\n",
       "                                       (&#x27;classifier&#x27;, LogisticRegression())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;classifier__C&#x27;: [0.1, 1, 10],\n",
       "                         &#x27;classifier__solver&#x27;: [&#x27;liblinear&#x27;, &#x27;saga&#x27;],\n",
       "                         &#x27;vectorizer__max_df&#x27;: [0.75, 0.85, 1.0],\n",
       "                         &#x27;vectorizer__min_df&#x27;: [1, 2, 3]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">estimator: Pipeline</label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                 TfidfVectorizer(lowercase=False,\n",
       "                                 tokenizer=&lt;function custom_tokenizer at 0x7f366dd240e0&gt;)),\n",
       "                (&#x27;classifier&#x27;, LogisticRegression())])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;TfidfVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfVectorizer(lowercase=False,\n",
       "                tokenizer=&lt;function custom_tokenizer at 0x7f366dd240e0&gt;)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression()</pre></div> </div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('vectorizer',\n",
       "                                        TfidfVectorizer(lowercase=False,\n",
       "                                                        tokenizer=<function custom_tokenizer at 0x7f366dd240e0>)),\n",
       "                                       ('classifier', LogisticRegression())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'classifier__C': [0.1, 1, 10],\n",
       "                         'classifier__solver': ['liblinear', 'saga'],\n",
       "                         'vectorizer__max_df': [0.75, 0.85, 1.0],\n",
       "                         'vectorizer__min_df': [1, 2, 3]},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Charger les données\n",
    "def load_data(filepath, label):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        data = [(line.strip(), label) for line in lines if line.strip()]\n",
    "    return data\n",
    "\n",
    "# Emplacements des fichiers\n",
    "bad_queries_path = 'badqueries.txt'\n",
    "good_queries_path = 'goodqueries.txt'\n",
    "\n",
    "# Charger et étiqueter les données\n",
    "bad_data = load_data(bad_queries_path, 1)  # Étiquette 1 pour les mauvaises requêtes\n",
    "good_data = load_data(good_queries_path, 0)  # Étiquette 0 pour les bonnes requêtes\n",
    "\n",
    "# Combinez les données dans un DataFrame\n",
    "all_data = pd.DataFrame(bad_data + good_data, columns=['query', 'label'])\n",
    "\n",
    "# Séparer les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_data['query'], all_data['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Définir une fonction de tokenisation personnalisée\n",
    "def custom_tokenizer(text):\n",
    "    token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b|\\/|<|>|script|alert|http|\\.exe|\\.jpg|=\")\n",
    "    return token_pattern.findall(text)\n",
    "\n",
    "# Pipeline avec Vectorizer et Logistic Regression\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(tokenizer=custom_tokenizer, lowercase=False, strip_accents=None)),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Paramètres à tester lors de la recherche en grille\n",
    "param_grid = {\n",
    "    'vectorizer__max_df': [0.75, 0.85, 1.0],\n",
    "    'vectorizer__min_df': [1, 2, 3],\n",
    "    'classifier__C': [0.1, 1, 10],\n",
    "    'classifier__solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "# Recherche en grille avec validation croisée\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'classifier__C': 10, 'classifier__solver': 'liblinear', 'vectorizer__max_df': 1.0, 'vectorizer__min_df': 1}\n",
      "Best Cross-Validation Accuracy: 0.9970923845213896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9972553254878593\n",
      "Confusion Matrix:\n",
      " [[258796    225]\n",
      " [   512   8987]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    259021\n",
      "           1       0.98      0.95      0.96      9499\n",
      "\n",
      "    accuracy                           1.00    268520\n",
      "   macro avg       0.99      0.97      0.98    268520\n",
      "weighted avg       1.00      1.00      1.00    268520\n",
      "\n",
      "Query: web4gfo.preprod.insee.fr:443/fr/graphique/affaires\n",
      "Prediction: Good Query\n",
      "Prediction Probability: [9.99358416e-01 6.41583778e-04]\n"
     ]
    }
   ],
   "source": [
    "# Afficher les meilleurs paramètres et la meilleure précision\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_}\")\n",
    "\n",
    "# Prédiction avec les meilleurs paramètres\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Afficher les métriques de performance\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Prédire avec une nouvelle query\n",
    "new_query = [\"web4gfo.preprod.insee.fr:443/fr/graphique/affaires\"]\n",
    "new_query_tfidf = best_model.named_steps['vectorizer'].transform(new_query)\n",
    "prediction = best_model.named_steps['classifier'].predict(new_query_tfidf)\n",
    "prediction_proba = best_model.named_steps['classifier'].predict_proba(new_query_tfidf)\n",
    "\n",
    "print(f\"Query: {new_query[0]}\")\n",
    "print(f\"Prediction: {'Bad Query' if prediction[0] == 1 else 'Good Query'}\")\n",
    "print(f\"Prediction Probability: {prediction_proba[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explication en Détail\n",
    "\n",
    "#### Pipeline avec Vectorizer et Logistic Regression\n",
    "\n",
    "Un pipeline en `scikit-learn` permet d'enchaîner plusieurs étapes de traitement des données et d'entraînement du modèle en un seul objet. Cela simplifie grandement le processus de transformation des données et d'entraînement du modèle, et assure que toutes les étapes sont appliquées de manière cohérente sur les ensembles d'entraînement et de test.\n",
    "\n",
    "```python\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(tokenizer=custom_tokenizer, lowercase=False, strip_accents=None)),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "```\n",
    "\n",
    "1. **Vectorizer** : La première étape du pipeline est le `TfidfVectorizer`. Ce transformateur convertit les requêtes textuelles en une matrice de caractéristiques en utilisant le TF-IDF (Term Frequency-Inverse Document Frequency). Il utilise une fonction de tokenisation personnalisée définie par `custom_tokenizer`.\n",
    "\n",
    "2. **Classifier** : La seconde étape du pipeline est le modèle de régression logistique (`LogisticRegression`). Ce modèle est utilisé pour la classification des requêtes en bonnes ou mauvaises.\n",
    "\n",
    "#### Paramètres à Tester lors de la Recherche en Grille\n",
    "\n",
    "La recherche en grille (`GridSearchCV`) permet de tester plusieurs combinaisons d'hyperparamètres pour trouver la meilleure configuration pour le modèle.\n",
    "\n",
    "```python\n",
    "param_grid = {\n",
    "    'vectorizer__max_df': [0.75, 0.85, 1.0],\n",
    "    'vectorizer__min_df': [1, 2, 3],\n",
    "    'classifier__C': [0.1, 1, 10],\n",
    "    'classifier__solver': ['liblinear', 'saga']\n",
    "}\n",
    "```\n",
    "\n",
    "1. **vectorizer__max_df** : Paramètre du `TfidfVectorizer` qui ignore les termes ayant une fréquence de document strictement supérieure au seuil donné (exprimé en proportion du nombre total de documents). Les valeurs testées sont 0.75, 0.85, et 1.0.\n",
    "\n",
    "2. **vectorizer__min_df** : Paramètre du `TfidfVectorizer` qui ignore les termes ayant une fréquence de document strictement inférieure au seuil donné. Les valeurs testées sont 1, 2, et 3.\n",
    "\n",
    "3. **classifier__C** : Paramètre de régularisation du modèle de régression logistique. Les valeurs testées sont 0.1, 1, et 10. Un plus grand C signifie une régularisation moindre.\n",
    "\n",
    "4. **classifier__solver** : Algorithme utilisé pour optimiser le modèle de régression logistique. Les valeurs testées sont `liblinear` et `saga`.\n",
    "\n",
    "#### Recherche en Grille avec Validation Croisée\n",
    "\n",
    "La validation croisée est une technique pour évaluer la performance d'un modèle de manière plus robuste en le testant sur plusieurs sous-ensembles des données.\n",
    "\n",
    "```python\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "1. **GridSearchCV** : Recherche les meilleures combinaisons d'hyperparamètres en utilisant la validation croisée (ici, 5 fois, `cv=5`).\n",
    "2. **scoring='accuracy'** : La mesure de performance utilisée pour évaluer les modèles est la précision.\n",
    "3. **n_jobs=-1** : Utilise tous les processeurs disponibles pour accélérer la recherche en grille.\n",
    "4. **fit(X_train, y_train)** : Entraîne le pipeline avec les différentes combinaisons d'hyperparamètres sur les données d'entraînement.\n",
    "\n",
    "#### Résultats\n",
    "\n",
    "```python\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_}\")\n",
    "\n",
    "# Prédiction avec les meilleurs paramètres\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "```\n",
    "\n",
    "##### Best Parameters\n",
    "```plaintext\n",
    "Best Parameters: {'classifier__C': 10, 'classifier__solver': 'liblinear', 'vectorizer__max_df': 1.0, 'vectorizer__min_df': 1}\n",
    "```\n",
    "\n",
    "Les meilleurs hyperparamètres trouvés par la recherche en grille sont :\n",
    "- **classifier__C** : 10 (Faible régularisation, ce qui permet au modèle de mieux s'ajuster aux données)\n",
    "- **classifier__solver** : `liblinear` (Un solver approprié pour des petits datasets)\n",
    "- **vectorizer__max_df** : 1.0 (Utilise tous les termes)\n",
    "- **vectorizer__min_df** : 1 (Ignore les termes apparaissant dans moins d'un document)\n",
    "\n",
    "##### Best Cross-Validation Accuracy\n",
    "```plaintext\n",
    "Best Cross-Validation Accuracy: 0.9970923845213896\n",
    "```\n",
    "\n",
    "La meilleure précision moyenne obtenue lors de la validation croisée est de 99.71%.\n",
    "\n",
    "##### Accuracy\n",
    "```plaintext\n",
    "Accuracy: 0.9972553254878593\n",
    "```\n",
    "\n",
    "La précision sur le jeu de données de test est de 99.73%, ce qui signifie que le modèle classifie correctement environ 99.73% des requêtes.\n",
    "\n",
    "##### Confusion Matrix\n",
    "```plaintext\n",
    "Confusion Matrix:\n",
    " [[258796    225]\n",
    " [   512   8987]]\n",
    "```\n",
    "\n",
    "La matrice de confusion montre :\n",
    "- **258796** vrais négatifs (bonnes requêtes correctement classées)\n",
    "- **225** faux positifs (bonnes requêtes incorrectement classées comme mauvaises)\n",
    "- **512** faux négatifs (mauvaises requêtes incorrectement classées comme bonnes)\n",
    "- **8987** vrais positifs (mauvaises requêtes correctement classées)\n",
    "\n",
    "##### Classification Report\n",
    "```plaintext\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      1.00      1.00    259021\n",
    "           1       0.98      0.95      0.96      9499\n",
    "\n",
    "    accuracy                           1.00    268520\n",
    "   macro avg       0.99      0.97      0.98    268520\n",
    "weighted avg       1.00      1.00      1.00    268520\n",
    "```\n",
    "\n",
    "- **Classe 0 (bonnes requêtes)** :\n",
    "  - **Précision** : 100% (presque toutes les bonnes requêtes sont correctement identifiées)\n",
    "  - **Rappel** : 100% (presque toutes les bonnes requêtes sont trouvées)\n",
    "  - **F1-score** : 100% (combinaison de précision et de rappel)\n",
    "  \n",
    "- **Classe 1 (mauvaises requêtes)** :\n",
    "  - **Précision** : 98% (98% des requêtes identifiées comme mauvaises sont réellement mauvaises)\n",
    "  - **Rappel** : 95% (95% des mauvaises requêtes sont correctement identifiées)\n",
    "  - **F1-score** : 96% (bonne balance entre précision et rappel)\n",
    "\n",
    "Le modèle est très performant, avec une haute précision et rappel pour les deux classes, montrant qu'il est capable de bien distinguer entre les bonnes et mauvaises requêtes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleur modèle sauvegardé sous le nom: best_logistic_regression_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import dill\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b|\\/|<|>|script|alert|http|\\.exe|\\.jpg|=\")\n",
    "    return token_pattern.findall(text)\n",
    "\n",
    "# Exporter le meilleur modèle\n",
    "best_model = grid_search.best_estimator_\n",
    "dill_file = \"best_logistic_regression_model.pkl\"\n",
    "with open(dill_file, 'wb') as file:\n",
    "    dill.dump(best_model, file)\n",
    "\n",
    "print(f\"Meilleur modèle sauvegardé sous le nom: {dill_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application aux données Splunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Path to the JSON file\n",
    "file_path = '/home/onyxia/work/ml-dl-cyber/Splunk/1716794372_291602.json'\n",
    "\n",
    "# Read the file line by line and parse each JSON object\n",
    "data = []\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "# Extract the 'result' part of each JSON object and create a dataset\n",
    "results = [entry['result'] for entry in data]\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Define a function to extract variables from the _raw column\n",
    "def extract_variables(raw_string):\n",
    "    # Define the regular expression pattern with named groups\n",
    "    pattern = re.compile(\n",
    "        # Match timestamp (e.g., \"May 24 14:33:29\")\n",
    "        r'(?P<timestamp>[A-Za-z]+\\s+\\d+\\s+\\d+:\\d+:\\d+)\\s+'\n",
    "        # Match host IP (e.g., \"172.22.7.248\")\n",
    "        r'(?P<host_ip>\\d+\\.\\d+\\.\\d+\\.\\d+)\\s+'\n",
    "        # Match any alphanumeric characters, dashes, or dots (e.g., \"PD-HLB01-ID261.dc2.sia.priv\")\n",
    "        r'[A-Za-z0-9\\-\\.]+\\s+'\n",
    "        # Match the info level (e.g., \"info\")\n",
    "        r'(?P<info>[a-z]+)\\s+'\n",
    "        # Match tmm with digits in brackets (e.g., \"tmm[20235]\")\n",
    "        r'tmm\\[\\d+\\]:\\s+'\n",
    "        # Match the rule description\n",
    "        r'Rule\\s+/Common/Irule-Syslog-splunk_v2\\s+<HTTP_RESPONSE>:\\s+'\n",
    "        # Match src_ip (e.g., \"src_ip=\"10.92.115.117%5\"\")\n",
    "        r'src_ip=\"(?P<src_ip>[^\"]+)\",'\n",
    "        # Match vip (e.g., \"vip=\"10.210.248.11%5\"\")\n",
    "        r'vip=\"(?P<vip>[^\"]+)\",'\n",
    "        # Match http_method (e.g., \"http_method=\"POST\"\")\n",
    "        r'http_method=\"(?P<http_method>[^\"]+)\",'\n",
    "        # Match http_host (e.g., \"http_host=\"web4gfo.preprod.insee.fr:443\"\")\n",
    "        r'http_host=\"(?P<http_host>[^\"]+)\",'\n",
    "        # Match http_uri (e.g., \"http_uri=\"/fr/graphique/affaires\"\")\n",
    "        r'http_uri=\"(?P<http_uri>[^\"]+)\",'\n",
    "        # Match http_url (e.g., \"http_url=\"web4gfo.preprod.insee.fr:443/fr/graphique/affaires\"\")\n",
    "        r'http_url=\"(?P<http_url>[^\"]+)\",'\n",
    "        # Match http_version (e.g., \"http_version=\"1.1\"\")\n",
    "        r'http_version=\"(?P<http_version>[^\"]+)\",'\n",
    "        # Match http_user_agent (e.g., \"http_user_agent=\"Mozilla/5.0 ...\"\")\n",
    "        r'http_user_agent=\"(?P<http_user_agent>[^\"]+)\",'\n",
    "        # Match http_content_type (e.g., \"http_content_type=\"application/x-www-form-urlencoded; charset=UTF-8\"\")\n",
    "        r'http_content_type=\"(?P<http_content_type>[^\"]+)\",'\n",
    "        # Match http_referrer (e.g., \"http_referrer=\"https://web4gfo.preprod.insee.fr/fr/accueil\"\")\n",
    "        r'http_referrer=\"(?P<http_referrer>[^\"]+)\",'\n",
    "        # Match req_start_time (e.g., \"req_start_time=\"2024/05/24 14:33:29\"\")\n",
    "        r'req_start_time=\"(?P<req_start_time>[^\"]+)\",'\n",
    "        # Match cookie (e.g., \"cookie=\"JSESSIONID\"\")\n",
    "        r'cookie=\"(?P<cookie>[^\"]*)\",'\n",
    "        # Match user (e.g., \"user=\"\"\")\n",
    "        r'user=\"(?P<user>[^\"]*)\",'\n",
    "        # Match virtual_server (e.g., \"virtual_server=\"/Common/pool_ppweb4gfoln071 0\"\")\n",
    "        r'virtual_server=\"(?P<virtual_server>[^\"]+)\",'\n",
    "        # Match bytes_in (e.g., \"bytes_in=\"68\"\")\n",
    "        r'bytes_in=\"(?P<bytes_in>\\d+)\",'\n",
    "        # Match res_start_time (e.g., \"res_start_time=\"2024/05/24 14:33:29\"\")\n",
    "        r'res_start_time=\"(?P<res_start_time>[^\"]+)\",'\n",
    "        # Match node (e.g., \"node=\"10.210.56.47%5\"\")\n",
    "        r'node=\"(?P<node>[^\"]+)\",'\n",
    "        # Match node_port (e.g., \"node_port=\"80\"\")\n",
    "        r'node_port=\"(?P<node_port>\\d+)\",'\n",
    "        # Match http_status (e.g., \"http_status=\"200\"\")\n",
    "        r'http_status=\"(?P<http_status>\\d+)\",'\n",
    "        # Match req_elapsed_time (e.g., \"req_elapsed_time=\"193\"\")\n",
    "        r'req_elapsed_time=\"(?P<req_elapsed_time>\\d+)\",'\n",
    "        # Match bytes_out (e.g., \"bytes_out=\"41523\"\")\n",
    "        r'bytes_out=\"(?P<bytes_out>\\d+)\"'\n",
    "    )\n",
    "    match = pattern.match(raw_string)\n",
    "    if match:\n",
    "        return match.groupdict()\n",
    "    return {}\n",
    "\n",
    "# Apply the function to the _raw column and expand the dictionaries into columns\n",
    "extracted_df = results_df['_raw'].apply(lambda x: pd.Series(extract_variables(x)))\n",
    "\n",
    "# Concatenate the extracted columns with the original DataFrame\n",
    "final_df = pd.concat([results_df, extracted_df], axis=1)\n",
    "\n",
    "#print(final_df.describe())\n",
    "#print(final_df[\"http_url\"])\n",
    "# Drop the original _raw column as it is no longer needed\n",
    "final_df = final_df.drop(columns=['_raw'])\n",
    "\n",
    "# Display all columns and their values one above the other\n",
    "#for col in final_df.columns:\n",
    "#    print(f\"Column: {col}\")\n",
    "#    print(final_df[col].to_string(index=False))\n",
    "#    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Optionally, save the final DataFrame to a CSV file\n",
    "final_df.to_csv('final_results_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "# Charger le modèle et le transformateur TF-IDF déjà entraînés\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "\n",
    "# Charger le fichier CSV contenant les nouvelles requêtes\n",
    "final_result_dataset_path = '/home/onyxia/work/ml-dl-cyber/1-Quick-win/Good_Bad_queries/final_results_dataset.csv'\n",
    "final_data = pd.read_csv(final_result_dataset_path)\n",
    "\n",
    "\n",
    "# Vérifier les premières lignes pour s'assurer que les données sont correctement chargées\n",
    "# print(final_data.head())\n",
    "\n",
    "\n",
    "\n",
    "# Définir la fonction de tokenisation personnalisée (si non déjà définie dans le contexte)\n",
    "def custom_tokenizer(text):\n",
    "    token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b|\\/|<|>|script|alert|http|\\.exe|\\.jpg|=\")\n",
    "    return token_pattern.findall(text)\n",
    "\n",
    "# Utiliser la colonne http_url pour les prédictions\n",
    "queries_to_predict = final_data['http_url']\n",
    "#print(queries_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer les nouvelles requêtes en vecteurs TF-IDF\n",
    "queries_tfidf = best_model.named_steps['vectorizer'].transform(queries_to_predict)\n",
    "\n",
    "# Effectuer les prédictions\n",
    "predictions = best_model.named_steps['classifier'].predict(queries_tfidf)\n",
    "prediction_probas = best_model.named_steps['classifier'].predict_proba(queries_tfidf)\n",
    "\n",
    "# Ajouter les prédictions et probabilités au DataFrame\n",
    "final_data['Prediction'] = predictions\n",
    "final_data['Prediction_Probability_Good'] = prediction_probas[:, 0]\n",
    "final_data['Prediction_Probability_Bad'] = prediction_probas[:, 1]\n",
    "\n",
    "# Afficher les résultats pour chaque requête\n",
    "#for index, row in final_data.iterrows():\n",
    "#    print(f\"Query: {row['http_url']}\")\n",
    "#    print(f\"Prediction: {'Bad Query' if row['Prediction'] == 1 else 'Good Query'}\")\n",
    "#    print(f\"Prediction Probability - Good: {row['Prediction_Probability_Good']:.4f}\")\n",
    "#    print(f\"Prediction Probability - Bad: {row['Prediction_Probability_Bad']:.4f}\")\n",
    "#    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Sauvegarder les résultats dans un nouveau fichier CSV\n",
    "final_data.to_csv('predicted_final_results_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Count  Percentage\n",
      "Prediction                   \n",
      "0            3103   99.582798\n",
      "1              13    0.417202\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Charger le fichier CSV contenant les nouvelles requêtes avec les prédictions\n",
    "final_result_dataset_path = '/home/onyxia/work/ml-dl-cyber/1-Quick-win/Good_Bad_queries/predicted_final_results_dataset.csv'\n",
    "final_data = pd.read_csv(final_result_dataset_path)\n",
    "\n",
    "# Faire un résumé statistique de la colonne 'Prediction'\n",
    "prediction_summary = final_data['Prediction'].value_counts()\n",
    "prediction_summary_percentage = final_data['Prediction'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Créer un DataFrame pour afficher les résultats\n",
    "summary_df = pd.DataFrame({\n",
    "    'Count': prediction_summary,\n",
    "    'Percentage': prediction_summary_percentage\n",
    "})\n",
    "\n",
    "# Afficher les résultats\n",
    "print(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URLs des requêtes prédites comme mauvaises (Prediction == 1):\n",
      "488     \"web4gfo.preprod.insee.fr:443/fr/graphique/aja...\n",
      "974     \"web4gfo.preprod.insee.fr:443/fr/graphique/aja...\n",
      "982     \"web4gfo.preprod.insee.fr:443/fr/graphique/aja...\n",
      "1077    \"web4gfo.preprod.insee.fr:443/en/graphique/aja...\n",
      "1079    \"web4gfo.preprod.insee.fr:443/en/graphique/aja...\n",
      "1085    \"web4gfo.preprod.insee.fr:443/en/graphique/aja...\n",
      "1123    \"web4gfo.preprod.insee.fr:443/en/graphique/aja...\n",
      "1126    \"web4gfo.preprod.insee.fr:443/en/graphique/aja...\n",
      "1146    \"web4gfo.preprod.insee.fr:443/en/graphique/aja...\n",
      "2101    \"web4gfo.preprod.insee.fr:443/fr/graphique/aja...\n",
      "2324    \"web4gfo.preprod.insee.fr:443/en/statistiques?...\n",
      "2325    \"web4gfo.preprod.insee.fr:443/langue/en?url=%2...\n",
      "2340    \"web4gfo.preprod.insee.fr:443/fr/statistiques?...\n",
      "Name: http_url, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Charger le fichier CSV contenant les nouvelles requêtes avec les prédictions\n",
    "final_result_dataset_path = '/home/onyxia/work/ml-dl-cyber/1-Quick-win/Good_Bad_queries/predicted_final_results_dataset.csv'\n",
    "final_data = pd.read_csv(final_result_dataset_path)\n",
    "\n",
    "# Filtrer les données pour lesquelles la prédiction est égale à 1\n",
    "predicted_bad_queries = final_data[final_data['Prediction'] == 1]\n",
    "\n",
    "# Afficher la colonne 'http_url' des requêtes filtrées\n",
    "bad_query_urls = predicted_bad_queries['http_url']\n",
    "\n",
    "print(\"URLs des requêtes prédites comme mauvaises (Prediction == 1):\")\n",
    "print(bad_query_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder les URLs filtrées dans un nouveau fichier CSV\n",
    "bad_query_urls.to_csv('bad_query_urls.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Itération sur le tokenizer\n",
    "\n",
    "Les différences entre le tokenizer précédent et le tokenizer amélioré résident principalement dans les motifs supplémentaires et les ajustements qui permettent de capturer une plus grande variété de tokens pertinents. Voici une comparaison détaillée :\n",
    "\n",
    "### Tokenizer Précédent :\n",
    "```python\n",
    "def custom_tokenizer(text):\n",
    "    # Utilisez une expression régulière pour considérer les mots et certains symboles spéciaux\n",
    "    token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b|\\/|<|>|script|alert|http|\\.exe|\\.jpg|=\")\n",
    "    return token_pattern.findall(text)\n",
    "```\n",
    "#### Motifs Capturés :\n",
    "- `(?u)\\b\\w\\w+\\b` : Capturait les mots de 2 caractères ou plus.\n",
    "- `\\/|<|>|script|alert|http|\\.exe|\\.jpg|=` : Capturait certains symboles spéciaux et sous-chaînes spécifiques comme `script`, `alert`, `http`, `.exe`, `.jpg`.\n",
    "\n",
    "### Tokenizer Amélioré :\n",
    "```python\n",
    "import re\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    # Utilisez une expression régulière améliorée pour capturer les mots, les chiffres, et les symboles spéciaux pertinents\n",
    "    token_pattern = re.compile(\n",
    "        r\"\"\"\n",
    "        (?u)\\b\\w\\w+\\b           # mots de 2 caractères ou plus\n",
    "        | \\d+                   # séquences de chiffres\n",
    "        | \\bhttps?://\\S+\\b      # URLs commençant par http ou https\n",
    "        | [\\/<>=&]              # caractères spéciaux courants dans les URL et les requêtes\n",
    "        | \\b\\w+\\.(com|org|net|fr|in)\\b  # domaines courants\n",
    "        \"\"\", re.VERBOSE)\n",
    "    return token_pattern.findall(text)\n",
    "```\n",
    "\n",
    "#### Motifs Capturés :\n",
    "- `(?u)\\b\\w\\w+\\b` : Capturait les mots de 2 caractères ou plus (comme dans la version précédente).\n",
    "- `\\d+` : Ajoute la capture des séquences de chiffres.\n",
    "- `\\bhttps?://\\S+\\b` : Ajoute la capture des URLs commençant par `http` ou `https`.\n",
    "- `[\\/<>=&]` : Capture les caractères spéciaux courants dans les URLs et les requêtes (maintien de certains symboles du tokenizer précédent).\n",
    "- `\\b\\w+\\.(com|org|net|fr|in)\\b` : Ajoute la capture des domaines courants (comme `.com`, `.org`, `.net`, `.fr`, `.in`).\n",
    "\n",
    "### Comparaison des Améliorations :\n",
    "1. **Captures de chiffres** : Le tokenizer amélioré peut maintenant capturer des séquences de chiffres (`\\d+`), ce qui est utile pour les dates, les quantités et d'autres informations numériques courantes dans les URLs et les textes.\n",
    "   \n",
    "2. **Capture des URLs complètes** : Le motif `\\bhttps?://\\S+\\b` permet de capturer les URLs complètes, contrairement à l'ancien motif qui ne capturait que la sous-chaîne `http`.\n",
    "\n",
    "3. **Domaines courants** : Le tokenizer amélioré capture les noms de domaines courants comme `.com`, `.org`, `.net`, `.fr`, et `.in`, ajoutant une granularité utile pour l'analyse des URLs.\n",
    "\n",
    "4. **Expression régulière plus lisible** : L'utilisation de `re.VERBOSE` permet de formater l'expression régulière avec des commentaires et des espaces, rendant le code plus lisible et maintenable.\n",
    "\n",
    "### Pourquoi ces changements ?\n",
    "- **Augmentation de la couverture** : Ces ajouts augmentent la couverture des tokens potentiellement pertinents, y compris des éléments spécifiques aux URLs et des chiffres.\n",
    "- **Réduction des faux positifs et négatifs** : En capturant des motifs plus spécifiques, le tokenizer réduit les chances de marquer incorrectement des requêtes légitimes comme suspectes.\n",
    "- **Meilleure analyse des données web** : Les nouvelles captures (comme les URLs complètes et les domaines courants) sont particulièrement utiles pour l'analyse de données web, ce qui est pertinent dans le contexte de vos données.\n",
    "\n",
    "Ces améliorations rendent le tokenizer plus robuste et adaptable à l'analyse des requêtes HTTP, tout en maintenant la simplicité et la lisibilité du code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
