{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "<img src=\"https://datascientest.fr/train/assets/logo_datascientest.png\" style=\"height:150px\">\n",
    "\n",
    "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
    "<h1><center>Masterclass ML2</center></h1>\n",
    "<h2><center>Interpr\u00e9tation de mod\u00e8les</center></h2>\n",
    "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
    "\n",
    "> Apr\u00e8s avoir appris \u00e0 utiliser diff\u00e9rents algorithmes de Machine Learning, entra\u00eener et \u00e9valuer tout type de mod\u00e8le, viendra le temps de votre premi\u00e8re mise en situation r\u00e9elle.<br> Un des points importants mais souvent minimis\u00e9, lorsque l'on enseigne la Data Science, est l'**interpr\u00e9tabilit\u00e9**.<br>\n",
    "> L'interpr\u00e9tabilit\u00e9 est la mesure dans laquelle un \u00eatre humain peut pr\u00e9dire de mani\u00e8re coh\u00e9rente le r\u00e9sultat d'un mod\u00e8le.\n",
    ">\n",
    "> Plus l'interpr\u00e9tabilit\u00e9 d'un mod\u00e8le de Machine Learning est \u00e9lev\u00e9e, plus il est facile pour un individu de comprendre le raisonnement derri\u00e8re certaines d\u00e9cisions ou pr\u00e9dictions. Un mod\u00e8le est plus facilement interpr\u00e9table qu'un autre si ses d\u00e9cisions sont plus faciles \u00e0 comprendre pour un humain. \n",
    ">\n",
    "> Lorsqu'ils s'attaquent aux probl\u00e8mes de Machine Learning, les sp\u00e9cialistes de donn\u00e9es ont souvent tendance \u00e0 se focaliser sur les mesures de performance des mod\u00e8les comme le taux de bonnes pr\u00e9dictions, la pr\u00e9cision/le rappel, la RMSE etc. C'est \u00e9galement le cas dans la plupart des concours et challenges de Machine Learning sur Internet.\n",
    ">\n",
    ">  Ces mesures de performances sont importantes, mais elles n'expliquent pas les d\u00e9cisions pr\u00e9dictives d'un mod\u00e8le. Au fil du temps, les performances peuvent diminuer car le mod\u00e8le n'est plus optimal par rapport \u00e0 divers changements de distribution des variables explicatives. Il est donc important de comprendre ce qui pousse un mod\u00e8le \u00e0 prendre certaines d\u00e9cisions afin d'identifier les features auxquels un mod\u00e8le serait tr\u00e8s sensible et dont il faut surveiller la distribution au fil du temps.\n",
    ">\n",
    "> Parfois un mod\u00e8le peut r\u00e9aliser de bonnes performances, mais apr\u00e8s d\u00e9ploiement, on peut se rendre compte qu'il ne correspond pas \u00e0 ce qu'on attendait.\n",
    ">\n",
    ">Prenons l'exemple classique o\u00f9 nous construisons un mod\u00e8le de classification de loups et de chiens. <br>\n",
    ">Les donn\u00e9es \u00e0 notre disposition sont simplement des images labellis\u00e9es de diff\u00e9rents types de chiens et de loups.\n",
    "\n",
    "<img src=\"https://datascientest.fr/train/assets/chien-loup.jpg\" style=\"height:150px\">\n",
    "\n",
    "> Notre mod\u00e8le obtient un tr\u00e8s bon score sur nos images et identifie avec beaucoup de succ\u00e8s les diff\u00e9rents loups parmis les chiens.\n",
    "Mais en utilisant une m\u00e9thode d'interpr\u00e9tation, nous constatons que notre mod\u00e8le ignore en fait le chien et le loup alors qu'il utilise uniquement les pixels d'arri\u00e8re-plan pour effectuer la classification. \n",
    "\n",
    "<img src=\"https://datascientest.fr/train/assets/chien-loup-2.jpg\" style=\"height:150px\">\n",
    "\n",
    ">En effet, dans nos images r\u00e9colt\u00e9es les loups et les chiens sont pr\u00e9sents dans des milieux compl\u00e8tement diff\u00e9rents. Les loups se trouvent principalement dans la nature (dans la neige, les for\u00eats, etc..) tandis que les chiens sont g\u00e9n\u00e9ralement photographi\u00e9s dans une habitation ou un parc am\u00e9nag\u00e9.\n",
    ">\n",
    "> L'interpr\u00e9tabilit\u00e9 est aussi importante d\u00e8s que les r\u00e9sultats d'un mod\u00e8le influent grandement sur des d\u00e9cisions importantes.\n",
    ">En entreprise par exemple, expliquer \u00e0 des \u00e9quipes non-initi\u00e9es le fonctionnement d'un mod\u00e8le pose toujours son lot de d\u00e9fis.<br> \n",
    "Cela conduit souvent \u00e0 un **sacrifice en termes de performance**. \n",
    "Dans la majorit\u00e9 des cas, les mod\u00e8les complexes comme les mod\u00e8les d'ensemble ou les r\u00e9seaux de neurones donnent g\u00e9n\u00e9ralement des performances meilleures et plus pr\u00e9cises. Mais dans certains domaines de l'industrie, notamment dans le monde de la finance, de l'assurance ou la banque, les Data Scientists finissent souvent par devoir utiliser des mod\u00e8les de Machine learning plus traditionnels (r\u00e9gression lin\u00e9aire ou arbres de d\u00e9cision) car l'interpr\u00e9tabilit\u00e9 est tr\u00e8s importante pour l'entreprise afin d'expliquer chaque d\u00e9cision prise par le mod\u00e8le.\n",
    ">\n",
    ">\n",
    ">Les mod\u00e8les dits complexes en Data Science, sont souvent per\u00e7us comme des bo\u00eetes noires, capables de retourner des pr\u00e9dictions par magie. Cependant, la r\u00e9alit\u00e9 est que sans une compr\u00e9hension raisonnable du fonctionnement des mod\u00e8les de Machine Learning, les projets du monde r\u00e9el peine \u00e0 r\u00e9ussir.<br> \n",
    "La majorit\u00e9 des projets de Data Science ont un aspect technique et un aspect commercial. Le Data Scientist se charge de la cr\u00e9ation d'un mod\u00e8le efficace, mais puisque le mod\u00e8le prendra finalement de nombreuses d\u00e9cisions importantes pour l'entreprise, ses coll\u00e8gues sont en droit de se demander comment lui **faire confiance**. Ainsi, depuis quelques ann\u00e9es beaucoup d'experts en Data Science se penchent sur la question de l'interpr\u00e9tabilit\u00e9 des mod\u00e8les.\n",
    ">\n",
    "> Les trois aspects les plus importants de l'interpr\u00e9tation des mod\u00e8les sont expliqu\u00e9s par les questions suivantes:\n",
    ">\n",
    "> > Qu'est-ce qui motive les pr\u00e9dictions du mod\u00e8le ? Nous devrions pouvoir interroger notre mod\u00e8le et d\u00e9couvrir les interactions cach\u00e9es entre les diff\u00e9rentes *features* pour avoir une id\u00e9e de celles qui pourraient \u00eatre les plus importantes dans les prises de d\u00e9cisions du mod\u00e8le. Cela permet de garantir **l'\u00e9quit\u00e9** du mod\u00e8le.\n",
    "> >\n",
    "> > Pourquoi le mod\u00e8le a-t-il pris une certaine d\u00e9cision ? Nous devrions \u00e9galement \u00eatre en mesure de valider et de justifier pourquoi certaines caract\u00e9ristiques cl\u00e9s ont \u00e9t\u00e9 \u00e0 l'origine de certaines d\u00e9cisions prises par un mod\u00e8le lors des pr\u00e9dictions. Cela permet de garantir la responsabilit\u00e9 et la **fiabilit\u00e9** du mod\u00e8le.\n",
    "> >\n",
    "> > Comment pouvons-nous faire confiance aux pr\u00e9dictions du mod\u00e8le ? Nous devons pouvoir \u00e9valuer et valider la fa\u00e7on dont un mod\u00e8le prend des d\u00e9cisions pour tout point de donn\u00e9es. Ainsi, on peut d\u00e9montrer et expliquer facilement que le mod\u00e8le fonctionne comme pr\u00e9vu. Cela permet de garantir la **transparence** du mod\u00e8le.\n",
    ">\n",
    ">\n",
    ">\n",
    ">Il existe plusieurs crit\u00e8res qui peuvent \u00eatre utilis\u00e9s pour cat\u00e9goriser les m\u00e9thodes d'interpr\u00e9tation des mod\u00e8les.\n",
    ">\n",
    "> L'interpr\u00e9tabilit\u00e9 est-elle intrins\u00e8que ou *post hoc* ? \n",
    ">\n",
    "> La m\u00e9thode est-elle sp\u00e9cifique ou ind\u00e9pendante du mod\u00e8le (model-agnostic)?  Les outils d'interpr\u00e9tation sp\u00e9cifiques \u00e0 un mod\u00e8le sont tr\u00e8s sp\u00e9cifiques aux m\u00e9thodes d'interpr\u00e9tation de mod\u00e8les intrins\u00e8ques qui d\u00e9pendent uniquement des capacit\u00e9s et des caract\u00e9ristiques de chaque mod\u00e8le.  Il peut s'agir de coefficients, de p-valeurs, de scores AIC relatifs \u00e0 un mod\u00e8le de r\u00e9gression, de r\u00e8gles issues d'un arbre de d\u00e9cision, etc. \n",
    "Les outils agnostiques de mod\u00e8les sont plus pertinents pour les m\u00e9thodes post hoc et peuvent \u00eatre utilis\u00e9s sur n'importe quel mod\u00e8le de Machine Learning.  Ces m\u00e9thodes agnostiques fonctionnent g\u00e9n\u00e9ralement en analysant les paires d'entr\u00e9es et de sorties de caract\u00e9ristiques.  Par d\u00e9finition, ces m\u00e9thodes n'ont acc\u00e8s \u00e0 aucun \u00e9l\u00e9ment interne du mod\u00e8le comme les poids, les contraintes ou les hypoth\u00e8ses. \n",
    ">\n",
    "> L'interpr\u00e9tation est-elle locale ou globale ?  La m\u00e9thode explique la pr\u00e9diction sur un seul individu/groupe d'individus ou le comportement entier du mod\u00e8le ?  \n",
    ">\n",
    "> Nous allons dans un premier temps pr\u00e9senter les techniques traditionnelles d'interpr\u00e9tabilit\u00e9 et leurs limites.<br>\n",
    "> Puis nous d\u00e9couvrirons un extrait des meilleures techniques d'interpr\u00e9tations actuelles.\n",
    ">\n",
    "> Pour cela, nous allons utiliser un jeu de donn\u00e9es simple avec des variables compr\u00e9hensibles, nous permettant de pr\u00e9dire pour certains individus leurs revenus annuels (plus ou moins de 50K$).\n",
    "\n",
    "* (a) On commence par importer les packages, modules et fonctions n\u00e9cessaires.\n",
    "\n",
    "\n",
    "* (b) On lit le fichier *'census_data.csv'* dans un DataFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, roc_auc_score, roc_curve, auc\n",
    "import shap\n",
    "import skater\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('census_data.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "* (c) Observons rapidement le jeu de donn\u00e9es, ainsi que la distribution de la variable cible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# On affiche les informations de jeu de donn\u00e9es\n",
    "display(df.info())\n",
    "\n",
    "# On affiche les 5 premi\u00e8res lignes de notre jeu de donn\u00e9es\n",
    "display(df.head())\n",
    "\n",
    "# On affiche la r\u00e9partition de notre variable cible\n",
    "sns.countplot(x=\"income\", data=df);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "* (d) S\u00e9parons la variable cible des *features*, puis encoder les variables cat\u00e9gorielles et cr\u00e9er un set d'entra\u00eenement et un set de test.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "X = df.drop(columns='income')\n",
    "y = df['income']\n",
    "\n",
    "X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "X_encoded.head()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, y, test_size=0.2, stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "* (e) Cr\u00e9ons et entra\u00eenons deux mod\u00e8les basiques : un arbre de d\u00e9cision et une r\u00e9gression logistique.\n",
    "\n",
    "\n",
    "* (f) Les deux mod\u00e8les seront entra\u00een\u00e9s puis \u00e9valu\u00e9s \u00e0 l'aide des m\u00e9triques habituelles.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "tree_clf = DecisionTreeClassifier(max_depth=3)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Validation Mean F1 Score: \", cross_val_score(\n",
    "    tree_clf, X_train, y_train, cv=5, scoring='f1_macro').mean())\n",
    "\n",
    "print(\"Validation Mean Accuracy: \", cross_val_score(\n",
    "    tree_clf, X_train, y_train, cv=5, scoring='accuracy').mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "* (g) Affichons le rapport de classification pour plus de d\u00e9tails.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "y_pred = tree_clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "* (h) Mettons \u00e0 l'\u00e9chelle les variables de notre mod\u00e8le avant d'entra\u00eener notre r\u00e9gression logistique.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "lr = LogisticRegression(solver='liblinear', max_iter=1000)\n",
    "\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Validation Mean F1 Score: \", cross_val_score(\n",
    "    lr, X_train, y_train, cv=5, scoring='f1_macro').mean())\n",
    "\n",
    "print(\"Validation Mean Accuracy: \", cross_val_score(\n",
    "    lr, X_train, y_train, cv=5, scoring='accuracy').mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "* (i) Nous entra\u00eenons le mod\u00e8le de r\u00e9gression logistique et nous affichons le rapport de classification. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "X_test_scaled = scaler.transform(X_test)\n",
    "y_pred = lr.predict(X_test_scaled)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "## Techniques traditionnelles et limites\n",
    "\n",
    "> Des techniques traditionnelles simples permettent de comprendre certains aspects des d\u00e9cisions prises par les mod\u00e8les entra\u00een\u00e9s.\n",
    "Ces techniques existantes et souvent utilis\u00e9es apr\u00e8s entra\u00eenement pour tenter d'expliquer un mod\u00e8le peuvent \u00eatre regroup\u00e9es en 2 cat\u00e9gories:\n",
    ">\n",
    "> * Les techniques g\u00e9n\u00e9rales *d'analyses et de visualisations exploratoires* comme l'analyse de corr\u00e9lation de variables ou la r\u00e9duction de dimension (ACP, AFM..).\n",
    ">\n",
    "> * Les techniques propres \u00e0 chaque classe d'algorithme, bas\u00e9es sur les param\u00e8tres des mod\u00e8les, comme l'affichage des arbres ou des coefficients.\n",
    ">\n",
    "> Il est possible d'afficher un arbre repr\u00e9sentant les d\u00e9cisions d'un mod\u00e8le gr\u00e2ce \u00e0 la fonction `plot_tree`.\n",
    "\n",
    "* (a) Affichons l'arbre de d\u00e9cision.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plt.figure(figsize=(15,12))\n",
    "plot_tree(tree_clf, feature_names = X_train.columns.tolist(), filled=True); #proportion=True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "> Ici, seules quelques variables seulement permettent de classer les individus. Lorsque le mod\u00e8le est simple et l'arbre peu profond, on obtient facilement une interpr\u00e9tation globale et locale du mod\u00e8le.\n",
    ">\n",
    "> En revanche, un arbre peu profond permet rarement d'obtenir de bonnes performances et de capturer toutes les interactions entre donn\u00e9es explicatives et variable cible. A l'inverse, un arbre tr\u00e8s profond devient moins \u00e9vident \u00e0 visualiser et \u00e0 interpr\u00e9ter.\n",
    ">\n",
    "> On note \u00e9galement que les mod\u00e8les non cr\u00e9\u00e9s \u00e0 base d'arbres de d\u00e9cisions ne peuvent \u00eatre interpr\u00e9t\u00e9s ainsi.\n",
    ">\n",
    "> De nombreux mod\u00e8les proposent de retourner l'importance de chaque variable (*feature importance*). Dans le cas d'un arbre de d\u00e9cision par exemple, la *feature importance* est calcul\u00e9e comme la diminution de l'impuret\u00e9 de chaque noeud o\u00f9 la feature est pr\u00e9sente, pond\u00e9r\u00e9 par la probabilit\u00e9 d'atteindre ce noeud. La probabilit\u00e9 d'atteindre le noeud peut \u00eatre calcul\u00e9e par le nombre d'\u00e9chantillons qui atteignent le noeud, divis\u00e9 par le nombre total d'\u00e9chantillons. <br>\n",
    "Plus cette valeur est \u00e9lev\u00e9e, plus la variable est consid\u00e9r\u00e9e importante pour le mod\u00e8le.\n",
    "\n",
    "* (b) Affichons les 3 variables les plus importantes pour le mod\u00e8le `tree_clf`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "feat_importances = pd.Series(\n",
    "    tree_clf.feature_importances_, index=X_train.columns)\n",
    "feat_importances.nlargest(3).plot(kind='barh');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "> Pour les mod\u00e8les d'arbres de d\u00e9cision, les *features importances* sont extr\u00eamement li\u00e9es \u00e0 l'arbre construit, et les arbres sont souvent tr\u00e8s instables. Au moindre changement dans le jeu d'entra\u00eenement les variables choisies peuvent \u00eatre remplac\u00e9es.\n",
    "> Pour d'autres mod\u00e8les, l'importance des variables est calcul\u00e9e de mani\u00e8re plus robuste, mais elle se base sur les probabilit\u00e9s pour chaque individu d'appartenir \u00e0 chaque classse. Or, tous les mod\u00e8les ne retournent pas de probabilit\u00e9s (comme les SVMs, par exemple).\n",
    ">\n",
    "\n",
    "* (c) Pour interpr\u00e9ter les mod\u00e8les de r\u00e9gression lin\u00e9aire, on peut afficher les coefficients associ\u00e9s \u00e0 chaque variable explicative.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "pd.Series(lr.coef_[0], X_train.columns).sort_values(ascending=False).plot(kind='barh', figsize=(4,8));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "> Les coefficients d'un mod\u00e8le de r\u00e9gression permettent d'avoir un aper\u00e7u des relations faites par le mod\u00e8le entre chaque variable explicative et la variable cible. Par exemple ici, plus `capital_gain` est elev\u00e9, plus l'individu a de chances de gagner plus de 50K dollars par mois.\n",
    ">\n",
    "> D'autres facteurs rentrent en jeu pour expliquer les pr\u00e9dictions du mod\u00e8le, et notamment la distribution des variables. \n",
    "> Souvent, l'analyse des coefficients ne suffit pas \u00e0 interpr\u00e9ter le mod\u00e8le, et l'ajout de divers tests statistiques permettent la plupart du temps de comprendre l'importance de chaque variable.\n",
    "\n",
    "\n",
    "## Techniques d'Interpr\u00e9tation de mod\u00e8les\n",
    "\n",
    "> Les techniques vues jusqu'ici sont certainement utiles pour essayer de mieux comprendre nos donn\u00e9es et leur utilit\u00e9 dans les d\u00e9cisions des mod\u00e8les.\n",
    "Cependant, elles sont assez limit\u00e9es et peu g\u00e9n\u00e9ralisables aux algorithmes plus performants et plus complexes. \n",
    ">\n",
    "> Il existe une grande vari\u00e9t\u00e9 de nouvelles techniques d'interpr\u00e9tation de mod\u00e8les qui tentent de r\u00e9pondre aux limites des techniques traditionnelles d'interpr\u00e9tation de mod\u00e8les et de combattre le compromis classique entre l'interpr\u00e9tabilit\u00e9 et la performance. Nous allons \u00e9tudier certaines de ces techniques et strat\u00e9gies en nous concentrant sur les techniques d'interpr\u00e9tations agnostiques des mod\u00e8les, car ces techniques sont les plus int\u00e9ressantes pour expliquer les mod\u00e8les les plus puissants.\n",
    ">\n",
    "> Les m\u00e9thodes que nous allons d\u00e9couvrir utilisent deux prinpaux packages : **Skater** et **Shap**.\n",
    ">\n",
    "> Pour illustrer ces packages, nous allons entra\u00eener un mod\u00e8le **XGBoost** consid\u00e9r\u00e9 par beaucoup comme une 'black box' inexplicable.\n",
    "\n",
    "* (a) Entra\u00eenons le mod\u00e8le xgboost sur nos donn\u00e9es. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Encodage de la variable cible en 0/1\n",
    "y_train = [0 if x == '<=50K' else 1 for x in y_train]\n",
    "y_test = [0 if x == '<=50K' else 1 for x in y_test]\n",
    "\n",
    "# Param\u00e8tres d'entra\u00eenement\n",
    "params = {'objective': 'binary:logistic', 'eval_metric':'logloss', 'max_depth': 6}\n",
    "\n",
    "# Conversion des jeux de donn\u00e9es en DMatrix\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest =  xgb.DMatrix(data = X_test, label = y_test)\n",
    "\n",
    "# Entra\u00eenement du mod\u00e8le\n",
    "bst = xgb.train(params, dtrain, 200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "* (b) Evaluons les diff\u00e9rents r\u00e9sultats du mod\u00e8le.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "probs = bst.predict(dtest)\n",
    "preds = [0 if x < 0.5 else 1 for x in probs]\n",
    "\n",
    "print(classification_report(y_test, preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "### Interpr\u00e9tation de mod\u00e8le avec Skater\n",
    "\n",
    ">Skater est un package pr\u00f4nant l'interpr\u00e9tation globale et locale de toutes formes de mod\u00e8les, afin d'aider \u00e0 construire des syst\u00e8mes de Machine Learning interpr\u00e9tables pour des cas d'utilisation dans le monde r\u00e9el.\n",
    ">\n",
    "> Skater utilise uniquement un jeu de donn\u00e9es, ainsi qu'une fonction de pr\u00e9diction entra\u00een\u00e9e sur ces donn\u00e9es pour ces interpr\u00e9tations.\n",
    "\n",
    "* (c) Nous utilisons `Interpretation` et `InMemoryModel` de la librairie Skater.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "from skater.core.explanations import Interpretation\n",
    "from skater.model import InMemoryModel\n",
    "\n",
    "interpreter = Interpretation(X_train, feature_names=X_train.columns)\n",
    "\n",
    "def predict_xg(x):\n",
    "    return pd.DataFrame(bst.predict(xgb.DMatrix(x)))\n",
    "\n",
    "model = InMemoryModel(predict_xg, examples = X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "> Skater permet d'afficher les *features importances* de mani\u00e8re agnostique au mod\u00e8le.<br>\n",
    ">L'impl\u00e9mentation de l'importance est bas\u00e9e sur un crit\u00e8re th\u00e9orique d'information, mesurant l'entropie dans le changement des pr\u00e9dictions, \u00e9tant donn\u00e9 une perturbation d'une variable donn\u00e9e. Plus les crit\u00e8res de d\u00e9cisions d'un mod\u00e8le d\u00e9pendent d'une variable, plus nous verrons les pr\u00e9dictions changer en fonction de la perturbation de cette variable.  La m\u00e9thode utilis\u00e9e par d\u00e9faut est la variance pr\u00e9diction, qui est la valeur absolue moyenne des changements dans les pr\u00e9dictions, compte tenu des perturbations des donn\u00e9es.\n",
    "\n",
    "* (d) Affichons le graphique de feature importance obtenue avec la librairie Skater. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (14,7)\n",
    "\n",
    "plots = interpreter.feature_importance.plot_feature_importance(\n",
    "    model, ascending=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "> La d\u00e9pendance partielle d\u00e9crit l'impact marginal d'une variable sur la pr\u00e9diction du mod\u00e8le, en maintenant constantes les autres caract\u00e9ristiques du mod\u00e8le. La d\u00e9riv\u00e9e de la d\u00e9pendance partielle d\u00e9crit l'impact d'une variable sur les pr\u00e9dictions.\n",
    ">\n",
    ">Le diagramme de d\u00e9pendance partielle (PD plot) montre l'effet marginal d'une caract\u00e9ristique sur le r\u00e9sultat pr\u00e9vu d'un mod\u00e8le pr\u00e9alablement entra\u00een\u00e9. Les PDP peuvent montrer si la relation entre la cible et une variable explicative est lin\u00e9aire, monotone ou plus complexe.\n",
    "\n",
    "* (e) Affichons le *PD plot* des variables ayant le plus d'importance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "interpreter.partial_dependence.plot_partial_dependence(['age'], model, grid_resolution=100,\n",
    "                                                       with_variance=True, figsize=(6, 4));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    ">Il semble que plus les individus sont ag\u00e9s plus ils ont de chances de gagner de d'argent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "interpreter.partial_dependence.plot_partial_dependence(['educational-num'], model, grid_resolution=50,\n",
    "                                                       grid_range=(0, 1),\n",
    "                                                       with_variance=True, figsize=(6, 4));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "> Sans surprise, plus le niveau d'\u00e9tudes est \u00e9lev\u00e9, plus les chances de gagner de d'argent sont grandes.\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "interpreter.partial_dependence.plot_partial_dependence(['capital-gain'], model, grid_resolution=50,\n",
    "                                                       grid_range=(0, 1), with_variance=True, figsize=(8, 4));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "> Il n'est pas non plus surprenant que plus le gain en capital est \u00e9lev\u00e9, plus les chances de gagner de l'argent sont grandes.<br>\n",
    ">Il y a une forte augmentation entre environ 5 000 er 8 000 dollars, puis apr\u00e8s ~ 15 000 dollars, les chances de gagner plus de 50K dollars par an sont tr\u00e8s proches de 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "interpreter.partial_dependence.plot_partial_dependence(['hours-per-week'], model, grid_resolution=50,\n",
    "                                                       grid_range=(0, 1), n_samples=23000,\n",
    "                                                       with_variance=True, figsize=(6, 4));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "> A partir de ~ 35h/semaine, les chances de gagner plus d'argent augmentent l\u00e9g\u00e8rement.\n",
    ">\n",
    "> Skater permet aussi une interpr\u00e9tation locale de plusieurs mani\u00e8res diff\u00e9rentes. \n",
    "> La  m\u00e9thode principale consiste \u00e0 utiliser l'estimateur de base pour comprendre le comportement d'une pr\u00e9diction unique en utilisant l'algorithme [**LIME**](https://christophm.github.io/interpretable-ml-book/lime.html).\n",
    ">\n",
    "> **LIME** est un nouvel algorithme pour expliquer localement le comportement de tout mod\u00e8le en utilisant des mod\u00e8les de substitution interpr\u00e9tables. Cette forme d'interpr\u00e9tabilit\u00e9  aide \u00e0 g\u00e9n\u00e9rer des explications qui sont localement vraies mais qui peuvent ne pas expliquer le comportement global du mod\u00e8le.<br>\n",
    "Fondamentalement, les explications LIME sont bas\u00e9es sur des mod\u00e8les de substitution locale. Ces mod\u00e8les de substitution sont des mod\u00e8les interpr\u00e9tables (comme un mod\u00e8le lin\u00e9aire ou un arbre de d\u00e9cision) qui sont appris \u00e0 partir de pr\u00e9dictions du mod\u00e8le original de la bo\u00eete noire. Mais au lieu d'essayer d'entra\u00eener un mod\u00e8le de substitution global, le LIME se concentre sur l'ajustement de mod\u00e8les de substitution locale pour expliquer pourquoi des pr\u00e9dictions ponctuelles ont \u00e9t\u00e9 faites.<br>\n",
    "\n",
    "<img src=\"https://datascientest.fr/train/assets/lime-fitting-1.png\">\n",
    "\n",
    "* (f) Affichons les explications locales de l'algorithme pour diff\u00e9rents individus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "from skater.core.local_interpretation.lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "# Nous reconstruisons le m\u00eame mod\u00e8le mais avec des numpy arrays pour \u00e9viter les probl\u00e8me avec LIME du au nomns des features\n",
    "\n",
    "xgc_np = xgb.XGBClassifier(n_estimators=1000, max_depth=6, base_score=0.5,\n",
    "                           objective='binary:logistic', eval_metric='logloss', use_label_encoder=False)\n",
    "xgc_np.fit(X_train.values, y_train)\n",
    "\n",
    "\n",
    "exp = LimeTabularExplainer(X_test.values, feature_names=list(X_test.columns), \n",
    "                           class_names=['$50K or less', 'More than $50K'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "* (g) Testons `LimeTabularExplainer` sur nos individus. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "doc_num = 13 # Tester avec diff\u00e9rents individus\n",
    "\n",
    "print('Actual Label:', y_test[doc_num])\n",
    "print('Predicted Label:', preds[doc_num])\n",
    "exp.explain_instance(X_test.iloc[doc_num].values, xgc_np.predict_proba).show_in_notebook()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "> La plus r\u00e9cente et novatrice m\u00e9thode de Skater pour l'interpr\u00e9tation de mod\u00e8le est l'utilisation de *Global Surrogate models*, qui sont des mod\u00e8les 'de substitution'.<br>\n",
    "N'importe quel mod\u00e8le *'black box'*, comme notre mod\u00e8le XGBoost contenant un millier d'arbres de d\u00e9cision, peut ainsi \u00eatre rendu interpr\u00e9table par un simple mod\u00e8le.\n",
    ">\n",
    ">Un mod\u00e8le de substitution global est un mod\u00e8le interpr\u00e9table qui est form\u00e9 pour effectuer une approximation de pr\u00e9dictions d'un mod\u00e8le *'black box'*. Nous pouvons tirer des conclusions sur notre mod\u00e8le en interpr\u00e9tant le mod\u00e8le de substitution. En clair : R\u00e9soudre l'interpr\u00e9tabilit\u00e9 du Machine Learning en utilisant plus de Machine Learning ! \n",
    ">\n",
    "\n",
    "* (h) Essayons de cr\u00e9er notre propre mod\u00e8le de substitution, \u00e0 partir des pr\u00e9dictions de notre mod\u00e8le XGBoost.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# R\u00e9cup\u00e9ration des pr\u00e9dictions\n",
    "\n",
    "probs = bst.predict(dtrain)\n",
    "preds = [0 if x<0.5 else 1 for x in probs]\n",
    "\n",
    "#Entra\u00eenement du mod\u00e8le\n",
    "sg_clf = DecisionTreeClassifier()\n",
    "sg_clf.fit(X_train, preds)\n",
    "\n",
    "# Scores du mod\u00e8le\n",
    "sg_clf.score(X_test, y_test)\n",
    "print(classification_report(y_test, sg_clf.predict(X_test)))\n",
    "\n",
    "#Comparaison des pr\u00e9dictions\n",
    "print(sum(preds!=sg_clf.predict(X_train)))\n",
    "\n",
    "# Comparaison des pr\u00e9dictions sur le jeu de test \n",
    "probs = bst.predict(dtest)\n",
    "preds = [0 if x<0.5 else 1 for x in probs]\n",
    "\n",
    "print(sum(preds!=sg_clf.predict(X_test))/len(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "> Le nouvel arbre de d\u00e9cison cr\u00e9\u00e9, atteignant une performance tr\u00e8s proche du mod\u00e8le XGBoost initial peut nous permettre d'expliquer les r\u00e9sultats de ce dernier, tant au niveau local que global.\n",
    "\n",
    "* (i) Affichons le nouvel arbre de d\u00e9cision.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 12))\n",
    "plot_tree(sg_clf, feature_names=X_train.columns.tolist(),\n",
    "          max_depth=3, filled=True);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "### Interpr\u00e9tation de mod\u00e8le avec SHAP\n",
    "\n",
    ">**SHAP** (**SH**apley **A**dditive ex**P**lanations) est un excellent package proposant une approche unifi\u00e9e pour expliquer le r\u00e9sultat de tout mod\u00e8le de Machine Learning. SHAP relie la th\u00e9orie des jeux aux explications locales, en se basant sur les valeurs de Shapley.\n",
    ">De mani\u00e8re g\u00e9n\u00e9rale, les pr\u00e9dictions des mod\u00e8les peuvent \u00eatre expliqu\u00e9es en supposant que chaque variable explicative est un \"joueur\" dans un jeu o\u00f9 la pr\u00e9diction est le paiement. La valeur de *Shapley* - une m\u00e9thode issue de la th\u00e9orie des jeux coop\u00e9ratifs - nous indique comment r\u00e9partir \u00e9quitablement le \"gain\" entre les variables. \n",
    ">\n",
    "> La valeur de Shapley, est une m\u00e9thode qui permet d'attribuer les paiements aux joueurs en fonction de leur contribution au paiement total. Les joueurs coop\u00e8rent au sein d'une coalition et obtiennent un certain gain de cette coop\u00e9ration.\n",
    ">Le \"jeu\" est la t\u00e2che de pr\u00e9diction pour une seule instance du jeu de donn\u00e9es.<br>\n",
    ">Le \"gain\" est la pr\u00e9diction r\u00e9elle pour cette instance moins la pr\u00e9diction moyenne de toutes les instances.<br>\n",
    "Les \"joueurs\" sont les valeurs pour chaque variable de l'instance, qui collaborent pour recevoir le gain (= pr\u00e9dire une certaine valeur).\n",
    ">\n",
    ">L'objectif est d'expliquer la diff\u00e9rence entre la pr\u00e9vision r\u00e9elle et la pr\u00e9vision moyenne.\n",
    "La valeur de Shapley est la contribution marginale moyenne de la valeur d'une variable sur toutes les coalitions possibles. Les coalitions sont essentiellement des combinaisons de variables qui sont utilis\u00e9es pour estimer la valeur Shapley d'une variable sp\u00e9cifique.<br>\n",
    ">La valeur de Shapley est la moyenne (pond\u00e9r\u00e9e) des contributions marginales. Lorsque nous assemblons la valeur de Shapley pour toutes les variables, nous obtenons la distribution compl\u00e8te de la pr\u00e9diction (moins la moyenne) parmi les valeurs des variables. \n",
    ">\n",
    ">SHAP est une am\u00e9lioration des valeurs de Shapley.<br>\n",
    "SHAP attribue \u00e0 chaque variable une valeur d'importance pour une pr\u00e9diction particuli\u00e8re.  En g\u00e9n\u00e9ral, les valeurs SHAP tentent d'expliquer la sortie d'un mod\u00e8le (fonction) comme une somme des effets de chaque variable. Les valeurs SHAP r\u00e9sultent de la moyenne de tous les ordres possibles. Il s'agit de la seule approche prouv\u00e9e scientifiquement par la th\u00e9orie des jeux comme coh\u00e9rente. \n",
    ">\n",
    "> <div class=\"alert alert-success\">\n",
    "<i class=\"fa fa-question-circle\"></i> &emsp; \n",
    "    > Les <b>TreeExplainer</b> permettent d'interpr\u00e9ter les diff\u00e9rents mod\u00e8les d'ensemble \u00e0 base d'arbres, de mani\u00e8re rapide gr\u00e2ce \u00e0 l'impl\u00e9mentation d'un algorithme sp\u00e9cifique.\n",
    "En revanche pour l'interpr\u00e9tation de mod\u00e8les complexes diff\u00e9rents ('GradientExplainer', 'DeepExplainer', 'KernelExplainer'), le calcul reste assez lent.\n",
    "</div>\n",
    "\n",
    "* (j) Analysons les pr\u00e9dictions de notre mod\u00e8le avec SHAP.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(bst)\n",
    "\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "print('Expected Value:', explainer.expected_value)\n",
    "\n",
    "pd.DataFrame(shap_values).head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "> La matrice obtenue contient les valeurs SHAP pour chaque individu et chaque variable.\n",
    "> Chaque ligne correspond \u00e0 la diff\u00e9rence entre la sortie du mod\u00e8le pour un individu et la valeur attendue de la sortie du mod\u00e8le (l'Expected value). Cette diff\u00e9rence nous aidera \u00e0 expliquer pourquoi le mod\u00e8le est enclin \u00e0 pr\u00e9dire le r\u00e9sultat d'une classe sp\u00e9cifique.\n",
    ">\n",
    "> SHAP permet d'afficher un graphique en barre classique d'importance des variables en fonction de leurs valeurs Shap, mais \u00e9galement un graphique permettant de visualiser la densit\u00e9 des valeurs SHAP pour chaque variable afin d'identifier l'impact de chaque variable sur la sortie du mod\u00e8le pour les individus dans l'ensemble de donn\u00e9es de validation. Les variables sont tri\u00e9es par la somme des valeurs SHAP sur tous les individus.<br>\n",
    "Lorsque des points se chevauchent sur une ligne, ils s'empilent pour montrer la densit\u00e9, et la couleur de chaque point repr\u00e9sente la valeur de la caract\u00e9ristique de cet individu.\n",
    "\n",
    "* (k) Affichons les features importances obtenues avec SHAP.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    ">Globalement, l'\u00e2ge, le statut marital ou le niveau d'\u00e9tude ont un impact total plus important sur le mod\u00e8le que la caract\u00e9ristique \"capital-gain\", mais pour les individus o\u00f9 le gain est important, il a plus d'impact que les variables cit\u00e9es plus haut. En d'autres termes, le gain en capital a un impact important sur quelques pr\u00e9dictions, tandis que l'\u00e2ge ou le statut marital ont un impact plus faible mais sur toutes les pr\u00e9dictions.\n",
    ">\n",
    "> Les graphiques de d\u00e9pendance de SHAP montrent l'effet d'une variable sur l'ensemble des donn\u00e9es. Ils affichent les valeurs d'une variable par rapport \u00e0 la valeur SHAP de cette variable sur un ensemble d'individus. Les diagrammes de d\u00e9pendance SHAP sont similaires aux diagrammes de d\u00e9pendance partielle, mais tiennent compte des effets d'interactions pr\u00e9sents dans les variables. Une seconde variable peut \u00eatre choisie pour colorer les points afin de mettre en \u00e9vidence les interactions possibles entre variables.\n",
    "\n",
    "* (l) Affichons des dependance plots de diff\u00e9rentes variables : l'\u00e2ge, le nombre d'ann\u00e9es d'\u00e9tudes et le nombre d'heures travaill\u00e9es par semaine. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"age\", shap_values, X_test, interaction_index= \"capital-gain\")\n",
    "\n",
    "shap.dependence_plot(\"educational-num\", shap_values, X_test, interaction_index= \"capital-gain\")\n",
    "\n",
    "shap.dependence_plot(\"hours-per-week\", shap_values, X_test, interaction_index= \"age\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "> SHAP permet d'afficher, au niveau local, un graphique montrant le raisonnement qui a pouss\u00e9 le mod\u00e8le a classer un individu dans telle classe, par rapport aux valeurs des variables les plus influentes.\n",
    "> Les variables augmentant la probabilit\u00e9 de gagner plus d'argent sont en rouge, celles diminuant la probabilit\u00e9 sont en bleu.\n",
    "\n",
    "* (m) Affichons ce graphique local pour quelques individus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "\n",
    "shap.force_plot(explainer.expected_value, shap_values[3,:], X_test.iloc[3,:])\n",
    "\n",
    "shap.force_plot(explainer.expected_value, shap_values[233,:], X_test.iloc[233,:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "> L'un des principaux avantages du SHAP est qu'il permet de construire des graphiques globaux interactifs qui peuvent visualiser et expliquer plusieurs pr\u00e9dictions \u00e0 la fois. Ici, nous visualisons les d\u00e9cisions de pr\u00e9dictions du mod\u00e8le pour les 1000 premiers \u00e9chantillons de donn\u00e9es de test.\n",
    "\n",
    "* (n) Affichons le graphique interactif propos\u00e9 par SHAP.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value, shap_values[:1000], X_test[:1000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "> Les packages Skater et Shap ont encore de nombreuses fonctions non explor\u00e9es ici. \n",
    "N'h\u00e9sitez pas \u00e0 aller regarder les documentations officielles respectives.\n",
    ">\n",
    "> A pr\u00e9sent, plus aucun mod\u00e8le de Machine Learning ne pourra vous faire peur, tous peuvent \u00eatre expliqu\u00e9s et interpr\u00e9tables par le plus grand nombre!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "owner": "DataScientest"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}